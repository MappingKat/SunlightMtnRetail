{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41896ce4",
   "metadata": {},
   "source": [
    "# 3. Pre-processing and Training Data Development\n",
    "\n",
    "* [3 Training Data](#2_Data_training_introduction)\n",
    "  * [3.1 Dummy Variables/One Hot Encoding for Categorical](#3.1_one_hot_encoding)\n",
    "  * [3.2 Standardize Numerical Data](#3.2_standardize)\n",
    "  * [3.3 Testing Training](#3.3_testing_training)\n",
    " * [3.2 Summary](#3.7_Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75904d9f",
   "metadata": {},
   "source": [
    "## Training Data <a href=\"#2_Data_training_introduction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a4f8d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Libraries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Loaded Libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13b26691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21396 entries, 0 to 21395\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Category        21396 non-null  object \n",
      " 1   Brand           18956 non-null  object \n",
      " 2   Description     21396 non-null  object \n",
      " 3   Keyword         18956 non-null  object \n",
      " 4   UPC             18970 non-null  object \n",
      " 5   MSRP            21396 non-null  float64\n",
      " 6   Quantity        21396 non-null  int64  \n",
      " 7   SKU             21396 non-null  object \n",
      " 8   Color           15387 non-null  object \n",
      " 9   Size            15806 non-null  object \n",
      " 10  StyleNumber     7000 non-null   object \n",
      " 11  StyleName       8881 non-null   object \n",
      " 12  ParentCategory  21396 non-null  object \n",
      "dtypes: float64(1), int64(1), object(11)\n",
      "memory usage: 2.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "products = pd.read_csv(\"../data/processed/products.csv\")\n",
    "\n",
    "# # New category list\n",
    "# new_categories = pd.read_csv(\"../data/processed/Sunlight-Categories.csv\")[\"Category\"].tolist() \n",
    "\n",
    "# Inspect data\n",
    "print(products.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68903cc",
   "metadata": {},
   "source": [
    "### I. Dummy Variables/One Hot Encoding for Categorical <a href=\"#3.1_one_hot_encoding\">\n",
    "In general, categorical features need to be transformed or encoded to be used in some machine learning models, as is the case for Logistic Regression. A common transformation is so-called dummy encoding, where each possible value of a feature becomes a new column, and a 1 is placed in that column if the data instance (a row of the data) contained that value, and a 0 is placed in that column otherwise.\n",
    "    \n",
    "Let's identify categorical variables that need dummy encoding and create dummy variables using OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7500bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21396 entries, 0 to 21395\n",
      "Data columns (total 13 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   Category        21396 non-null  object  \n",
      " 1   Brand           18956 non-null  category\n",
      " 2   Description     21396 non-null  string  \n",
      " 3   Keyword         18956 non-null  object  \n",
      " 4   UPC             18970 non-null  object  \n",
      " 5   MSRP            21396 non-null  float64 \n",
      " 6   Quantity        21396 non-null  int64   \n",
      " 7   SKU             21396 non-null  object  \n",
      " 8   Color           15387 non-null  string  \n",
      " 9   Size            15806 non-null  string  \n",
      " 10  StyleNumber     7000 non-null   object  \n",
      " 11  StyleName       8881 non-null   object  \n",
      " 12  ParentCategory  21396 non-null  category\n",
      "dtypes: category(2), float64(1), int64(1), object(6), string(3)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert non-categorical columns to categorical format\n",
    "products['Brand'] = products['Brand'].astype('category')\n",
    "products['Color'] = products['Color'].astype('string')\n",
    "products['Size'] = products['Size'].astype('string')\n",
    "products['Description'] = products['Category'].astype('string')\n",
    "products['ParentCategory'] = products['ParentCategory'].astype('category')\n",
    "\n",
    "dummy_categories = pd.get_dummies(pd.Series(new_categories), prefix='category')\n",
    "products.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c287d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dummy variables with original dataset\n",
    "# Assuming your original dataset is stored in a DataFrame called df\n",
    "df = pd.concat([products, dummy_categories], axis=1)# Scale numerical fields using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_fields = ['MSRP']  # Add other numerical fields as needed\n",
    "df[numerical_fields] = scaler.fit_transform(df[numerical_fields])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c28af8",
   "metadata": {},
   "source": [
    "So far, we have scaled the one numerical MSRP variable and dummy coded the NEW Categories field. Next, lets convert the categorical features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f531f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data have  21396  rows and  13  columns\n",
      "\n",
      "column names: \n",
      "\n",
      "Category\n",
      "Brand\n",
      "Description\n",
      "Keyword\n",
      "UPC\n",
      "MSRP\n",
      "Quantity\n",
      "SKU\n",
      "Color\n",
      "Size\n",
      "StyleNumber\n",
      "StyleName\n",
      "ParentCategory\n"
     ]
    }
   ],
   "source": [
    "categorical_features = ['Brand',\n",
    "                        'Size',\n",
    "                        'Color', \n",
    "                        'Description',\n",
    "                        'ParentCategory',\n",
    "                        'Category']\n",
    "dflog = pd.get_dummies(products, columns = categorical_features)\n",
    "\n",
    "print('The data have ', products.shape[0], ' rows and ', products.shape[1], ' columns\\n')\n",
    "print('column names: \\n')\n",
    "print('\\n'.join(list(products.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a85c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9    ...  137  138  139  \\\n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "142  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "143  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "144  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "146  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "     140  141  142  143  144  145  146  \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  \n",
      "142  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "143  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "144  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "145  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "146  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[147 rows x 147 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Convert the list to a numpy array\n",
    "new_categories_array = np.array(new_categories)\n",
    "\n",
    "# Reshape the array to have a single column\n",
    "new_categories_array = new_categories_array.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding for new categories\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_features = pd.DataFrame(encoder.fit_transform(new_categories_array))\n",
    "\n",
    "# Encoded features now contain columns for each new category\n",
    "print(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96f74d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cca5bff",
   "metadata": {},
   "source": [
    "### II. Standardize Numerical Data <a href=\"#3.2_standardize\">\n",
    "    \n",
    "Standardize the magnitude of numeric features using a scaler (MSRP, Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f3bc687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical fields using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "numerical_fields = ['MSRP']  # Add other numerical fields as needed\n",
    "df[numerical_fields] = scaler.fit_transform(df[numerical_fields])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e086331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scale numerical features\n",
    "# scaled_numerical_cols = pd.DataFrame(df[numerical_fields], columns=numerical_fields.columns)\n",
    "\n",
    "# Combine scaled numerical columns with non-scaled features\n",
    "scaled_features = pd.concat([scaled_numerical_cols, features.drop(['MSRP'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c18da",
   "metadata": {},
   "source": [
    "### III. Training and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b077c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaled_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split data into training and testing sets (80% training, 20% testing)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(scaled_features, target, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scaled_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you have:\n",
    "# - X_train: Training features (scaled)\n",
    "# - X_test: Testing features (scaled)\n",
    "# - y_train: Training target variable\n",
    "# - y_test: Testing target variable\n",
    "\n",
    "# It's ready for training and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7ed53",
   "metadata": {},
   "source": [
    "#### Proportion of classes\n",
    "When building classification models, it is always a good idea to know right away the number of samples per class, proportionally to the total number of samples. First we get the counts of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329909bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = products['ParentCategory'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_percentages = pd.Series([(x / products.shape[0]) * 100.00 for x in class_counts])\n",
    "class_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4971e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(class_counts.index, class_counts)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xlabel('Category')\n",
    "ax.set_title('Category Distribution',\n",
    "              fontsize = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0302aeb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m encoded_features\u001b[38;5;241m.\u001b[39mcorr()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot heatmap\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m      6\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(correlation_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCorrelation Heatmap of Encoded Features\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = encoded_features.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Encoded Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a82da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3face",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d030904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
