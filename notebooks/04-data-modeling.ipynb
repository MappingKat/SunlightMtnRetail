{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ecc23",
   "metadata": {},
   "source": [
    "# 4. Data Modeling\n",
    "* [4 Introduction](#2_Introduction)\n",
    "* [4.1 Summary](#4.1_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import fasttext\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from tabulate import tabulate\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv('../data/processed/cleaned-products.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71be94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = products['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e8cc96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Filter the dataset based on the selected categories\n",
    "data = products[products['Category'].isin(categories)]\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "data['Description'] = data['Description'].fillna('')\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(description, stop_words):\n",
    "    words = description.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the 'Description' column\n",
    "data['Description'] = data['Description'].apply(lambda x: remove_stop_words(x, stop_words))\n",
    "\n",
    "# Remove rows with empty 'Description'\n",
    "data = data[data['Description'].str.strip() != '']\n",
    "\n",
    "# Filter out categories with fewer than 10 instances\n",
    "category_counts = data['Category'].value_counts()\n",
    "valid_categories = category_counts[category_counts >= 10].index\n",
    "data = data[data['Category'].isin(valid_categories)]\n",
    "\n",
    "# Check if there are any remaining data after filtering\n",
    "if data.empty:\n",
    "    raise ValueError(\"No data left after filtering categories with fewer than 10 instances\")\n",
    "\n",
    "# Define a CountVectorizer instance with the list of stopwords for BoW\n",
    "vectorizer_bow = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define a TfidfVectorizer instance with the list of stopwords for TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Text preprocessing for 'Description' using BoW\n",
    "description_bow = vectorizer_bow.fit_transform(data['Description'])\n",
    "\n",
    "# Text preprocessing for 'Description' using TF-IDF\n",
    "description_tfidf = vectorizer_tfidf.fit_transform(data['Description'])\n",
    "\n",
    "# Encode categorical features 'Vendor' and 'Category'\n",
    "encoder_vendor = OneHotEncoder(sparse_output=False)\n",
    "encoded_vendor = encoder_vendor.fit_transform(data[['Vendor']])\n",
    "\n",
    "encoder_category = OneHotEncoder(sparse_output=False)\n",
    "encoded_category = encoder_category.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features for BoW and TF-IDF\n",
    "combined_features_bow = np.hstack((description_bow.toarray(), encoded_vendor, encoded_category))\n",
    "combined_features_tfidf = np.hstack((description_tfidf.toarray(), encoded_vendor, encoded_category))\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['Category'])  # Fit LabelEncoder on the entire dataset\n",
    "\n",
    "# Split data into train and test sets, ensuring stratified distribution\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(combined_features_bow, data['Category'], test_size=0.2, random_state=42, stratify=data['Category'])\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(combined_features_tfidf, data['Category'], test_size=0.2, random_state=42, stratify=data['Category'])\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "}\n",
    "\n",
    "# Define an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"XGBoost\":\n",
    "        # Train XGBoost model\n",
    "        model.fit(X_train_bow, y_train_encoded)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_encoded = model.predict(X_test_bow)\n",
    "    elif name == \"Decision Tree\":\n",
    "        # Train Decision Tree model\n",
    "        model.fit(X_train_bow, y_train_encoded)\n",
    "        \n",
    "        # Convert class names to a list\n",
    "        class_names = label_encoder.classes_.tolist()\n",
    "        \n",
    "        # Convert feature names to a list\n",
    "        feature_names = vectorizer_bow.get_feature_names_out().tolist() + \\\n",
    "                        encoder_vendor.get_feature_names_out(input_features=['Vendor']).tolist() + \\\n",
    "                        encoder_category.get_feature_names_out(input_features=['Category']).tolist()\n",
    "\n",
    "        # Plot the Decision Tree\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(model, feature_names=feature_names, class_names=class_names, filled=True, rounded=True, fontsize=10)\n",
    "        plt.title(\"Decision Tree for Product Classification\")\n",
    "        plt.show()\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_encoded = model.predict(X_test_bow)\n",
    "    else:\n",
    "        # Train other models\n",
    "        if name == \"Naive Bayes\":\n",
    "            model.fit(X_train_tfidf, y_train_encoded)\n",
    "            y_pred_encoded = model.predict(X_test_tfidf)\n",
    "        else:\n",
    "            model.fit(X_train_bow, y_train_encoded)\n",
    "            y_pred_encoded = model.predict(X_test_bow)\n",
    "\n",
    "    # Decode the predicted labels back to original category names\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append([name, f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"])\n",
    "\n",
    "# Print table\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = products[products['Category'].isin(categories)]\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "data['Description'] = data['Description'].fillna('')\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Encode categorical features 'Brand' and 'Category'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_brand = encoder.fit_transform(data[['Vendor']])\n",
    "encoded_category = encoder.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features\n",
    "features = pd.concat([pd.DataFrame(encoded_brand), pd.DataFrame(encoded_category)], axis=1)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['ParentCategory'] = label_encoder.fit_transform(data['ParentCategory'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['ParentCategory'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "# Decode predictions\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "y_test_decoded = label_encoder.inverse_transform(y_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_decoded, y_pred)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_decoded, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900cac5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea51dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Filter the dataset based on the selected categories\n",
    "data = products[products['Category'].isin(categories)]\n",
    "\n",
    "# Fill NaN values with an empty string\n",
    "data['Description'] = data['Description'].fillna('')\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(description, stop_words):\n",
    "    words = description.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Apply the function to the 'Description' column\n",
    "data['Description'] = data['Description'].apply(lambda x: remove_stop_words(x, stop_words))\n",
    "\n",
    "# Remove rows with empty 'Description'\n",
    "data = data[data['Description'].str.strip() != '']\n",
    "\n",
    "# Filter out categories with fewer than 2 instances\n",
    "category_counts = data['Category'].value_counts()\n",
    "valid_categories = category_counts[category_counts >= 2].index\n",
    "data = data[data['Category'].isin(valid_categories)]\n",
    "\n",
    "# Define a CountVectorizer instance with the list of stopwords for BoW\n",
    "vectorizer_bow = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Text preprocessing for 'Description' using BoW\n",
    "description_bow = vectorizer_bow.fit_transform(data['Description'])\n",
    "\n",
    "# Encode categorical features 'Vendor' and 'Category'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded_brand = encoder.fit_transform(data[['Vendor']])\n",
    "encoded_category = encoder.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features for BoW\n",
    "combined_features_bow = np.hstack((description_bow.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(data['Category'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(combined_features_bow, data['Category'], test_size=0.2, random_state=42, stratify=data['Category'])\n",
    "\n",
    "y_train_encoded = label_encoder.transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308e232",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Doc2Vec is a modeling technique developed as an extension of Word2Vec, which is widely used for natural language processing (NLP) tasks. While Word2Vec learns continuous vector representations (word embeddings) for words in a text corpus, Doc2Vec extends this idea to learn embeddings for entire documents or sentences.\n",
    "\n",
    "In Doc2Vec, each document is represented as a fixed-length vector, similar to how words are represented in Word2Vec. This vector representation captures the semantic meaning of the document in a continuous vector space. The model is trained in an unsupervised manner, where it learns to predict the context of words or documents within a given window size. \n",
    "\n",
    "There are two main architectures for Doc2Vec:\n",
    "\n",
    "1. **Distributed Memory (DM)**: In this architecture, the model learns to predict the next word in a context given a fixed-length vector representation of the document and the context words. This architecture is similar to the Skip-gram model in Word2Vec.\n",
    "\n",
    "2. **Distributed Bag of Words (DBOW)**: In this architecture, the model learns to predict words in a context given only the fixed-length vector representation of the document. The document vector is randomly sampled from the document during training, and the model learns to predict words in the context of this vector. This architecture is similar to the Continuous Bag of Words (CBOW) model in Word2Vec.\n",
    "\n",
    "Doc2Vec models have been widely used for various NLP tasks, including document classification, sentiment analysis, information retrieval, and clustering. They provide a way to represent documents in a continuous vector space, enabling efficient similarity calculations and downstream tasks.\n",
    "Doc2Vec object does not have a fit attribute. This is because Doc2Vec does not follow the same fitting process as traditional machine learning models like logistic regression or decision trees.\n",
    "\n",
    "With Doc2Vec, you typically build the vocabulary and train the model separately from the fit and predict workflow used in supervised learning models. You first build the vocabulary using the build_vocab method and then train the model using the train method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5de996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and preprocess descriptions\n",
    "products['Processed_Description'] = products['Description'].apply(preprocess_text)\n",
    "\n",
    "# Convert descriptions to TaggedDocument objects\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(products['Processed_Description'])]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "products['Category_Encoded'] = label_encoder.fit_transform(products['Category'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(products['Processed_Description'], products['Category_Encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Infer vectors for descriptions\n",
    "X_train_vectors = [model.infer_vector(doc) for doc in X_train]\n",
    "X_test_vectors = [model.infer_vector(doc) for doc in X_test]\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Predict categories for test data\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Add the predicted categories to the DataFrame\n",
    "products.loc[X_test.index, 'Predicted_Category'] = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "results = [[\"Doc2Vec\", f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"]]\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fe471",
   "metadata": {},
   "source": [
    "Doc2Vec may perform poorly on short product descriptions for several reasons:\n",
    "\n",
    "Lack of Sufficient Context: Doc2Vec relies on learning vector representations for documents based on their surrounding context. Short product descriptions may not provide enough context for the model to capture the semantic meaning accurately. As a result, the learned document embeddings may not effectively represent the underlying information in the descriptions.\n",
    "\n",
    "Limited Vocabulary: Short descriptions may contain a limited vocabulary, leading to fewer training examples for the model to learn from. Doc2Vec models benefit from a diverse range of words and phrases to build robust document embeddings. If the vocabulary in the descriptions is limited, the model may struggle to generalize well to unseen data.\n",
    "\n",
    "Semantic Variability: Short descriptions may contain ambiguous or vague language, making it challenging for the model to infer the correct category. Since Doc2Vec learns continuous vector representations based on the context of words, it may struggle with capturing nuanced semantic meanings from short and ambiguous text.\n",
    "\n",
    "Data Sparsity: Short descriptions may result in sparse document representations, especially if they contain only a few words or phrases. Sparse representations may not adequately capture the information needed for accurate categorization, leading to suboptimal performance.\n",
    "\n",
    "To address these issues, it's essential to consider alternative approaches such as incorporating additional features, using different pre-trained embeddings, or leveraging domain-specific knowledge to enhance the performance of the classification task for short product descriptions. Additionally, experimenting with different model architectures and hyperparameters may also help improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667752a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products.dropna(subset=['Category'])\n",
    "products['Category'] = products['Category'].astype(str)\n",
    "\n",
    "# Create an empty list to store the parent categories\n",
    "parent_categories = []\n",
    "for category in products['Category']:\n",
    "    parts = category.split(\"-\", 1)\n",
    "    parent_category = parts[0] if len(parts) > 1 else category  # Use original if no \"-\"\n",
    "    if parent_category == 'X':\n",
    "        parent_category = 'Cross Country'\n",
    "    parent_categories.append(parent_category)\n",
    "    \n",
    "\n",
    "# Add the new parent category to the DataFrame\n",
    "products['ParentCategory'] = parent_categories\n",
    "\n",
    "print(products['ParentCategory'].nunique, products['Category'].nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e80996",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "FastText is a library developed by Facebook AI Research for efficient learning of word representations and sentence classification. It's an extension of the Word2Vec model that also considers subword information. Here's how FastText works for classification:\n",
    "\n",
    "* Word Representations: FastText learns continuous representations for words in a text corpus. It represents each word as a vector in a high-dimensional space, capturing semantic and syntactic information.\n",
    "\n",
    "* Subword Information: Unlike traditional word embeddings, FastText also considers subword information by breaking words into smaller n-grams (character sequences). This allows FastText to handle out-of-vocabulary words and capture morphological similarities.\n",
    "\n",
    "* Text Classification: FastText can be used for text classification tasks, such as categorizing product descriptions into predefined categories. It learns a classifier based on the word embeddings and predicts the most likely category for a given input text.\n",
    "\n",
    "To train a FastText model for classification, you can follow these steps:\n",
    "\n",
    "* Prepare Data: Organize your product descriptions and corresponding categories into a suitable format for training. Each training example should consist of a text description and its corresponding category label.\n",
    "\n",
    "* Preprocess Text: Clean and preprocess the text data, including tokenization, lowercasing, and removing stopwords or special characters.\n",
    "\n",
    "* Training: Train the FastText model using the preprocessed text data. You can specify parameters such as the dimensionality of word vectors, the context window size, and the number of training epochs.\n",
    "\n",
    "* Evaluation: Evaluate the trained model using a separate validation dataset to assess its performance in categorizing product descriptions accurately. You can calculate metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "* Prediction: Once the model is trained and evaluated, you can use it to predict the category labels for new product descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71e37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing and removing punctuation\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the \"Description\" field\n",
    "data['Processed_Description'] = data['Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "model.build_vocab(corpus_iterable=data['Processed_Description'])\n",
    "model.train(corpus_iterable=data['Processed_Description'], total_examples=len(data['Processed_Description']), epochs=model.epochs)\n",
    "\n",
    "# Convert descriptions to average word vectors\n",
    "X_fasttext = np.array([np.mean([model.wv[word] for word in words if word in model.wv] or [np.zeros(100)], axis=0) for words in data['Processed_Description']])\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['Category'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fasttext, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict categories for test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Add results for FastText model\n",
    "results.append([\"FastText\", f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"])\n",
    "\n",
    "# Print updated table\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d018892",
   "metadata": {},
   "source": [
    "FastText might not have performed as well as some other models in this classification task for a few reasons:\n",
    "\n",
    "Complexity of the Task: FastText is effective for capturing semantic meaning and relationships between words, especially in longer texts. However, if the product descriptions are short and lack enough context, FastText may not be able to capture sufficient semantic information to accurately classify the categories.\n",
    "\n",
    "* Data Size: FastText typically performs better with larger amounts of text data. If the dataset used for training is relatively small, FastText may not have had enough examples to learn robust representations of the product categories.\n",
    "\n",
    "* Quality of Preprocessing: The preprocessing steps applied to the product descriptions could impact FastText's performance. If the preprocessing steps removed important information or introduced noise into the data, it could have affected the model's ability to learn meaningful representations.\n",
    "\n",
    "* Parameter Tuning: FastText has several hyperparameters that can influence its performance, such as vector size, window size, and epochs. If these hyperparameters were not tuned effectively for the specific task and dataset, the model's performance could suffer.\n",
    "\n",
    "* Class Imbalance: If the distribution of categories in the dataset is imbalanced, meaning some categories have significantly fewer examples than others, FastText may struggle to accurately classify the minority classes.\n",
    "\n",
    "Overall, while FastText is a powerful tool for natural language processing tasks, its performance can vary depending on the specific characteristics of the dataset and the task at hand. In the case of short product descriptions and category classification, other models like logistic regression, decision trees, and random forests might outperform FastText due to their simplicity and ability to capture patterns in smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to combined features\n",
    "pca = PCA(n_components=2)  # You can choose 2 or 3 components for 2D or 3D visualization\n",
    "combined_features_pca = pca.fit_transform(combined_features_bow)\n",
    "\n",
    "# Assuming y_pred_encoded and combined_features_pca are your arrays\n",
    "print(\"Shape of y_pred_encoded:\", y_pred_encoded.shape)\n",
    "print(\"Shape of combined_features_pca:\", combined_features_pca.shape)\n",
    "\n",
    "if y_pred_encoded.shape[0] != combined_features_pca.shape[0]:\n",
    "    # Reshape y_pred_encoded to match the number of rows in combined_features_pca\n",
    "    y_pred_encoded = np.resize(y_pred_encoded, (combined_features_pca.shape[0],))\n",
    "\n",
    "# Now, both arrays should have the same number of rows\n",
    "print(\"Shape of y_pred_encoded after adjustment:\", y_pred_encoded.shape)\n",
    "\n",
    "# # Plot the transformed features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(combined_features_pca[:, 0], combined_features_pca[:, 1], c=y_pred_encoded, cmap='viridis', alpha=0.5)\n",
    "plt.title('PCA Visualization of Model Predictions')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Predicted Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a5281",
   "metadata": {},
   "source": [
    "The data visualization shows a two-dimensional principal component analysis (PCA) plot where the  x-axis is labeled \"Principal Component 1\" and the y-axis is labeled \"Principal Component 2\". The text at the top of the plot says \"PCA Visualization of Model Predictions\".  There are also labels for \"Predicted Category\"  along the color bar on the right.  Unfortunately, it is difficult to draw specific conclusions from this visualization because the  number of data points and the color scale are not provided.\n",
    "\n",
    "Here are some general observations about PCA visualizations:\n",
    "\n",
    "PCA is a dimensionality reduction technique that is often used to visualize high-dimensional data. It reduces the number of dimensions by finding new components, called principal components, that capture the most variance in the data.\n",
    "In a PCA visualization, each data point is projected onto the new principal components. The resulting plot can be used to see how the data points are clustered or grouped together.\n",
    "In the context of product categorization, a PCA visualization could be used to see how well a model is able to categorize products into different categories. The data points would represent the products, and the color would represent the predicted category. If the model is doing a good job of categorization, the data points for each category would be clustered together in the PCA plot.\n",
    "\n",
    "However, it is important to keep in mind that PCA visualizations can be difficult to interpret, especially for high-dimensional data.  If you are interested in learning more about PCA visualizations, you can search for  \"PCA visualization interpretation: https://innovationyourself.com/master-machine-learning-with-pca/\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf05a7",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "To get a tangible sense of what these models are doing, let's export a file to see the actual versus predicted values. This will also give us insight into which categories are difficult to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for pandas DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # Display full width of columns\n",
    "pd.set_option('display.max_rows', None)      # Display all rows\n",
    "\n",
    "# Create a DataFrame to display the test data output with Description and Category\n",
    "test_output = pd.DataFrame({\n",
    "    'Description': data.loc[y_test.index, 'Description'].values,\n",
    "    'Actual_Category': label_encoder.inverse_transform(y_test_encoded),\n",
    "    'Predicted_Category': label_encoder.inverse_transform(y_pred_encoded)\n",
    "})\n",
    "\n",
    "# Export the test data output to a CSV file\n",
    "test_output.to_csv('../data/processed/full_output.csv', index=False)\n",
    "\n",
    "# Print a message to confirm the export\n",
    "print(\"Test data output exported to '../data/processed/full_output.csv' successfully.\")\n",
    "\n",
    "# Reset display options to default after printing\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1f00c",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce9cd",
   "metadata": {},
   "source": [
    "Summary of Product Category Classification Results:\n",
    "\n",
    "* Logistic Regression: Achieved an accuracy of 0.99 and precision of 0.99. Logistic Regression is a linear model that works well for binary classification tasks. It performed exceptionally well in this scenario, likely due to its simplicity and ability to capture linear relationships between features and target classes.\n",
    "\n",
    "* Decision Tree: Also achieved high accuracy and precision of 0.99 and 1, respectively. Decision Trees are known for their interpretability and ability to handle non-linear relationships in the data. In this case, the Decision Tree model effectively learned complex decision boundaries, leading to accurate predictions.\n",
    "\n",
    "* Naive Bayes: Achieved an accuracy of 0.93 and precision of 0.94. Naive Bayes classifiers are based on the assumption of independence between features, which might not hold true for text data with short descriptions. Despite this limitation, Naive Bayes performed reasonably well but may struggle with capturing subtle dependencies between words in the descriptions.\n",
    "\n",
    "* Random Forest: Achieved high accuracy of 0.99 and precision of 1. Random Forest is an ensemble learning method that combines multiple decision trees to improve performance. It excelled in capturing complex relationships in the data and mitigating overfitting, leading to high accuracy and precision.\n",
    "\n",
    "* Support Vector Machine (SVM): Achieved an accuracy of 0.98 and precision of 0.99. SVMs are effective for high-dimensional data and can learn complex decision boundaries. The slightly lower accuracy compared to other models could be due to the choice of kernel and parameters, which may need further tuning for optimal performance.\n",
    "\n",
    "* XGBoost: Achieved an accuracy of 0.98 and precision of 0.99. XGBoost is a powerful gradient boosting algorithm that builds a sequence of decision trees to improve predictive performance. It performed well in this scenario but may require more computational resources and tuning compared to other models.\n",
    "\n",
    "**Factors Influencing Accuracy and Precision:**\n",
    "\n",
    "The accuracy and precision of the models can vary based on several factors, including:\n",
    "\n",
    "* Short Descriptions: Short product descriptions may contain limited information, making it challenging for models to accurately classify categories. Models that can effectively capture the essence of the descriptions, such as Decision Trees and Random Forests, may perform better in this scenario.\n",
    "\n",
    "* Category Complexity: Some categories may be inherently more challenging to classify due to ambiguity or similarity with other categories. Models with the ability to capture subtle differences between categories, such as Logistic Regression and Support Vector Machines, may excel in such cases.\n",
    "\n",
    "* Feature Representation: The choice of feature representation, such as Bag-of-Words or TF-IDF, can impact model performance. Models like Naive Bayes, which rely on feature independence assumptions, may struggle with capturing the semantic meaning of words in short descriptions compared to more advanced models like Word Embeddings (e.g., FastText and Doc2Vec).\n",
    "\n",
    "Overall, while all models achieved high accuracy and precision, the choice of model depends on the specific characteristics of the dataset and the desired balance between interpretability and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989b562",
   "metadata": {},
   "source": [
    "Additional Resources: \n",
    "- [blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)\n",
    "- [textcategorizer](https://spacy.io/api/textcategorizer)\n",
    "- [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
    "- [vec regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2747753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5003330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
