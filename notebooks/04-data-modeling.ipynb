{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ecc23",
   "metadata": {},
   "source": [
    "# 4. Data Modeling\n",
    "* [4 Training Data](#2_Data_training_introduction)\n",
    "  * [4.1 Dummy Variables/One Hot Encoding for Categorical](#3.1_one_hot_encoding)\n",
    "  * [4.2 Standardize Numerical Data](#3.2_standardize)\n",
    "  * [4.3 Testing Training](#3.3_testing_training)\n",
    " * [4.2 Summary](#3.7_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a48204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/homebrew/anaconda3/lib/python3.11/site-packages (2.0.3)\r\n",
      "Requirement already satisfied: fasttext in /opt/homebrew/anaconda3/lib/python3.11/site-packages (0.9.2)\r\n",
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.4)\r\n",
      "Requirement already satisfied: scipy in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from xgboost) (1.10.1)\r\n",
      "Requirement already satisfied: pybind11>=2.2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from fasttext) (2.11.1)\r\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from fasttext) (68.0.0)\r\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\r\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\r\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "\n",
    "!/opt/homebrew/anaconda3/bin/python -m pip install xgboost fasttext nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e8745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2661 entries, 9454 to 4138\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   Brand           2661 non-null   category\n",
      " 1   Description     2661 non-null   string  \n",
      " 2   Keyword         2661 non-null   object  \n",
      " 3   UPC             2661 non-null   object  \n",
      " 4   MSRP            2661 non-null   float64 \n",
      " 5   Quantity        2661 non-null   int64   \n",
      " 6   SKU             2661 non-null   object  \n",
      " 7   Color           2661 non-null   string  \n",
      " 8   Size            2661 non-null   string  \n",
      " 9   StyleNumber     2661 non-null   object  \n",
      " 10  StyleName       2661 non-null   object  \n",
      " 11  ParentCategory  2661 non-null   string  \n",
      "dtypes: category(1), float64(1), int64(1), object(5), string(4)\n",
      "memory usage: 259.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "%store -r products\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "#print(products) \n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cec2e4",
   "metadata": {},
   "source": [
    "Resources: \n",
    "- [blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)\n",
    "- [textcategorizer](https://spacy.io/api/textcategorizer)\n",
    "- [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
    "- [vec regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f12db1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'s', 'wasn', 'here', 'a', \"it's\", 'no', 'it', 'of', 'myself', 'against', 'while', 'from', 'under', 'why', 'such', 'we', \"shouldn't\", 'doesn', 'our', 'that', 're', 'some', 'just', 'as', \"hasn't\", 'mustn', 'yourself', 'above', \"hadn't\", 'so', 'to', 'y', 'my', 've', 'most', 'other', 'ourselves', \"mightn't\", 'wouldn', \"isn't\", 'haven', 'which', 'mightn', 'ma', 'have', 'are', 'was', 'they', 'whom', 'didn', 'an', \"you'll\", 'i', 'do', 'very', \"should've\", \"didn't\", 'should', 'won', \"that'll\", 'did', 'and', 'down', 'same', 'aren', 'each', 'his', 'its', 'o', 'herself', 'hasn', 'until', \"mustn't\", 'out', 'll', 'shouldn', 'before', \"couldn't\", 'at', 'or', 'after', 'how', \"wasn't\", 'be', \"weren't\", 'where', 'doing', 'by', 'between', 'the', 'in', 't', 'me', 'both', 'into', 'who', 'because', 'too', 'can', 'what', 'weren', \"won't\", 'will', 'more', \"she's\", 'does', 'himself', \"you'd\", 'through', 'all', 'her', 'being', 'd', 'when', \"shan't\", 'during', \"you've\", 'any', 'were', 'had', 'few', \"you're\", 'about', 'is', 'up', 'couldn', 'yourselves', 'he', 'own', 'themselves', 'now', 'nor', 'again', 'yours', 'than', 'these', \"wouldn't\", 'their', 'them', 'isn', 'over', 'then', 'has', 'further', 'ain', 'she', 'hadn', \"needn't\", 'm', \"haven't\", 'shan', 'for', 'only', 'but', 'not', 'hers', 'your', \"don't\", 'off', 'ours', 'itself', 'there', 'once', 'below', \"aren't\", 'those', 'theirs', 'don', 'this', 'am', 'if', 'needn', 'on', 'you', 'been', \"doesn't\", 'him', 'with', 'having'} instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstop_words)\n\u001b[1;32m     23\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39mstop_words)\n\u001b[0;32m---> 24\u001b[0m description_bow \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m description_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Encode categorical features\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# encoder = OneHotEncoder(sparse_output=False)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# encoded_brand = encoder.fit_transform(data[['Brand']])\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# encoded_category = encoder.fit_transform(data[['Category']])\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Combine features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1141\u001b[0m )\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:637\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    630\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \n\u001b[1;32m    632\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     validate_parameter_constraints(\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m    640\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m    641\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got {'s', 'wasn', 'here', 'a', \"it's\", 'no', 'it', 'of', 'myself', 'against', 'while', 'from', 'under', 'why', 'such', 'we', \"shouldn't\", 'doesn', 'our', 'that', 're', 'some', 'just', 'as', \"hasn't\", 'mustn', 'yourself', 'above', \"hadn't\", 'so', 'to', 'y', 'my', 've', 'most', 'other', 'ourselves', \"mightn't\", 'wouldn', \"isn't\", 'haven', 'which', 'mightn', 'ma', 'have', 'are', 'was', 'they', 'whom', 'didn', 'an', \"you'll\", 'i', 'do', 'very', \"should've\", \"didn't\", 'should', 'won', \"that'll\", 'did', 'and', 'down', 'same', 'aren', 'each', 'his', 'its', 'o', 'herself', 'hasn', 'until', \"mustn't\", 'out', 'll', 'shouldn', 'before', \"couldn't\", 'at', 'or', 'after', 'how', \"wasn't\", 'be', \"weren't\", 'where', 'doing', 'by', 'between', 'the', 'in', 't', 'me', 'both', 'into', 'who', 'because', 'too', 'can', 'what', 'weren', \"won't\", 'will', 'more', \"she's\", 'does', 'himself', \"you'd\", 'through', 'all', 'her', 'being', 'd', 'when', \"shan't\", 'during', \"you've\", 'any', 'were', 'had', 'few', \"you're\", 'about', 'is', 'up', 'couldn', 'yourselves', 'he', 'own', 'themselves', 'now', 'nor', 'again', 'yours', 'than', 'these', \"wouldn't\", 'their', 'them', 'isn', 'over', 'then', 'has', 'further', 'ain', 'she', 'hadn', \"needn't\", 'm', \"haven't\", 'shan', 'for', 'only', 'but', 'not', 'hers', 'your', \"don't\", 'off', 'ours', 'itself', 'there', 'once', 'below', \"aren't\", 'those', 'theirs', 'don', 'this', 'am', 'if', 'needn', 'on', 'you', 'been', \"doesn't\", 'him', 'with', 'having'} instead."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "import fasttext\n",
    "\n",
    "# Load data\n",
    "data = products\n",
    "\n",
    "# Text preprocessing for 'Description'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "vectorizer = CountVectorizer(stop_words=stop_words)\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "description_bow = vectorizer.fit_transform(data['Description'])\n",
    "description_tfidf = tfidf_vectorizer.fit_transform(data['Description'])\n",
    "\n",
    "# Encode categorical features\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "# encoded_brand = encoder.fit_transform(data[['Brand']])\n",
    "# encoded_category = encoder.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features\n",
    "combined_features_bow = np.hstack((description_bow.toarray(), encoded_brand, encoded_category))\n",
    "combined_features_tfidf = np.hstack((description_tfidf.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(combined_features_bow, data['Category'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(combined_features_tfidf, data['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Doc2Vec\": gensim.models.Doc2Vec(vector_size=100, min_count=2, epochs=40),\n",
    "    \"FastText\": fasttext.train_supervised(input=description_tfidf, epoch=25, wordNgrams=2),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"Doc2Vec\":\n",
    "        # Train Doc2Vec model\n",
    "        model.build_vocab(data['Description'])\n",
    "        X_train_doc2vec = [model.infer_vector(doc.split()) for doc in data['Description'][:int(0.8 * len(data))]]\n",
    "        X_test_doc2vec = [model.infer_vector(doc.split()) for doc in data['Description'][int(0.8 * len(data)):]]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_doc2vec, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_doc2vec)\n",
    "    elif name == \"FastText\":\n",
    "        # Train FastText model\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "    else:\n",
    "        # Train other models\n",
    "        model.fit(X_train_bow, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_bow)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6e17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51fc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
