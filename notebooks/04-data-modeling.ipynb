{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ecc23",
   "metadata": {},
   "source": [
    "# 4. Data Modeling\n",
    "* [4 Introduction](#2_Introduction)\n",
    "* [4.1 Summary](#4.1_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18e8745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Category         Brand  \\\n",
      "3                Clothing - Winter - Outerwear     Obermeyer   \n",
      "10        Clothing - Winter - Outerwear - Mens     Obermeyer   \n",
      "15               Ski Hardgoods - Boots - Women       Nordica   \n",
      "18              Accessories - Winter - Goggles  Smith Optics   \n",
      "21                Ski Hardgoods - Skis - Women       Nordica   \n",
      "...                                        ...           ...   \n",
      "21369                    Ski Hardgoods - Poles       Salomon   \n",
      "21370                     Accessories - Summer        DAKINE   \n",
      "21379            Ski Hardgoods - Skis - Junior        Armada   \n",
      "21385           Accessories - Winter - Goggles    Airblaster   \n",
      "21393  Accessories - Winter - Gloves & Mittens        Armada   \n",
      "\n",
      "                                             Description       Keyword  \\\n",
      "3                           Jette Jacket Glacier melt 12     Obermeyer   \n",
      "10                Obermeyer - Keystone Pant - Black - LS     Obermeyer   \n",
      "15     Nordica - Speedmachine 75 W Ski Boots - Black/...       Nordica   \n",
      "18     Smith - I/O MAG Goggles - Sunrise, ChromaPop S...  Smith Optics   \n",
      "21                 Santa Ana 93 Flat Blue - Rasperry 158       Nordica   \n",
      "...                                                  ...           ...   \n",
      "21369       Salomon - Kaloo Ski Poles - Blue - Jr 105cm        Salomon   \n",
      "21370                  Dakine - Gripper - Deep Teal - OS        DAKINE   \n",
      "21379                  Arv 84 135 - 149 Joram Roukes 142        Armada   \n",
      "21385  Airblaster - Air Goggle - Chrome Lens - BGG - ...    Airblaster   \n",
      "21393                       Tremor Mitt Graphite X-Large        Armada   \n",
      "\n",
      "                UPC    MSRP  Quantity           SKU  \\\n",
      "3      888555767674  299.00         1  JET243078952   \n",
      "10     888555573336  109.50         0  KEY19725340S   \n",
      "15     888341789040  400.00         0   SPE39922955   \n",
      "18     716736827513  270.00         2  IOM31217987S   \n",
      "21     888341757582  750.00         0  SAN291544758   \n",
      "...             ...     ...       ...           ...   \n",
      "21369  193128282905   25.00         1  KAL187735925   \n",
      "21370  194626485249   25.00         0  GRI17177111S   \n",
      "21379  842339154463  375.00         0  ARV258490412   \n",
      "21385  847678218593  140.00         0  AIR33552290S   \n",
      "21393  842339162734   69.95         0  TRE24594218E   \n",
      "\n",
      "                               Color     Size  StyleNumber  \\\n",
      "3                       Glacier melt       12        11210   \n",
      "10                             Black       LS        25102   \n",
      "15       Black - Anthracite - Purple     23.5  050H4803735   \n",
      "18     Sunrise | ChromaPop Sun Black       OS       M00427   \n",
      "21                   Blue - Rasperry      158  0A031800001   \n",
      "...                              ...      ...          ...   \n",
      "21369                           Blue      105    L41174600   \n",
      "21370                       DEEPTEAL       OS     10003409   \n",
      "21379                   Joram Roukes      142    RA0000216   \n",
      "21385                            BGG       OS    AB24G_003   \n",
      "21393                       Graphite  X-Large    R00410030   \n",
      "\n",
      "                               StyleName  ParentCategory  \\\n",
      "3                           Jette Jacket       Clothing    \n",
      "10                         Keystone Pant       Clothing    \n",
      "15                     SPEEDMACHINE 75 W  Ski Hardgoods    \n",
      "18                               I/O MAG    Accessories    \n",
      "21                   SANTA ANA 93 (flat)  Ski Hardgoods    \n",
      "...                                  ...             ...   \n",
      "21369                       KALOO JUNIOR  Ski Hardgoods    \n",
      "21370                            GRIPPER    Accessories    \n",
      "21379                 ARV 84 (135 - 149)  Ski Hardgoods    \n",
      "21385  AIR GOGGLE CHROME LENS+SPARE LENS    Accessories    \n",
      "21393                        Tremor Mitt    Accessories    \n",
      "\n",
      "                                   Processed_Description  Category_Encoded  \\\n",
      "3                     [jette, jacket, glacier, melt, 12]                53   \n",
      "10                [obermeyer, keystone, pant, black, ls]                55   \n",
      "15     [nordica, speedmachine, 75, w, ski, boots, bla...                86   \n",
      "18     [smith, i/o, mag, goggles, sunrise, chromapop,...                17   \n",
      "21           [santa, ana, 93, flat, blue, rasperry, 158]                91   \n",
      "...                                                  ...               ...   \n",
      "21369      [salomon, kaloo, ski, poles, blue, jr, 105cm]                87   \n",
      "21370                  [dakine, gripper, deep, teal, os]                 0   \n",
      "21379            [arv, 84, 135, 149, joram, roukes, 142]                89   \n",
      "21385  [airblaster, air, goggle, chrome, lens, bgg, o...                17   \n",
      "21393                  [tremor, mitt, graphite, x-large]                16   \n",
      "\n",
      "                                Predicted_Category  \n",
      "3                                              NaN  \n",
      "10                                             NaN  \n",
      "15                                             NaN  \n",
      "18                                             NaN  \n",
      "21                                             NaN  \n",
      "...                                            ...  \n",
      "21369                                          NaN  \n",
      "21370                                          NaN  \n",
      "21379      Accessories - Winter - Gloves & Mittens  \n",
      "21385  Accessories - Winter - Hats, Hoods, Collars  \n",
      "21393      Accessories - Winter - Gloves & Mittens  \n",
      "\n",
      "[4435 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "# FUTURE TO DO: Store files so that you have a checkpoint; check guided capstone; easier to debug; \n",
    "%store -r products\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71be94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "import fasttext\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from tabulate import tabulate\n",
    "\n",
    "print('Libraries Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f3c323b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for the old categories\n",
    "categories = [\n",
    "    \"Accessories - Summer\",\n",
    "    \"Accessories - Summer - Bike Protection\",\n",
    "    \"Accessories - Summer - Cameras\",\n",
    "    \"Accessories - Summer - Hats & Ball Caps\",\n",
    "    \"Accessories - Summer - Helmets\",\n",
    "    \"Accessories - Summer - Helmets - Adult\",\n",
    "    \"Accessories - Summer - Helmets - Kids\",\n",
    "    \"Accessories - Summer - Helmets - Mens\",\n",
    "    \"Accessories - Summer - Helmets - Parts\",\n",
    "    \"Accessories - Summer - Helmets - Womens\",\n",
    "    \"Accessories - Summer - Hydration Packs/Bladders/filters\",\n",
    "    \"Accessories - Summer - Media\",\n",
    "    \"Accessories - Summer - Mirrors\",\n",
    "    \"Accessories - Summer - Nutrition\",\n",
    "    \"Accessories - Summer - Nutrition - Jerky\",\n",
    "    \"Accessories - Summer - Nutrition - Supplements\",\n",
    "    \"Accessories - Summer - Nutrition - Waffles\",\n",
    "    \"Accessories - Summer - Protective Wear\",\n",
    "    \"Accessories - Summer - Pumps & CO2\",\n",
    "    \"Accessories - Summer - Rear Racks\",\n",
    "    \"Accessories - Summer - Repair Stands\",\n",
    "    \"Accessories - Summer - Smartphone Carrier\",\n",
    "    \"Accessories - Summer - Storage\",\n",
    "    \"Accessories - Summer - Sunglasses/Goggles\",\n",
    "    \"Accessories - Summer - Trailers\",\n",
    "    \"Accessories - Summer - Training Wheels\",\n",
    "    \"Accessories - Winter\",\n",
    "    \"Accessories - Winter - Apres & Casual Footwear\",\n",
    "    \"Accessories - Winter - AT/Splitboard Accessories\",\n",
    "    \"Accessories - Winter - Bags\",\n",
    "    \"Accessories - Winter - Baselayers\",\n",
    "    \"Accessories - Winter - Baselayers - Kids\",\n",
    "    \"Accessories - Winter - Beanies\",\n",
    "    \"Accessories - Winter - Boot Accessories\",\n",
    "    \"Accessories - Winter - Car Racks\",\n",
    "    \"Accessories - Winter - Gloves & Mittens\",\n",
    "    \"Accessories - Winter - Goggle Covers\",\n",
    "    \"Accessories - Winter - Goggles\",\n",
    "    \"Accessories - Winter - Hats, Hoods, Collars\",\n",
    "    \"Accessories - Winter - Helmets\",\n",
    "    \"Accessories - Winter - Impulse\",\n",
    "    \"Accessories - Winter - Kid Gear\",\n",
    "    \"Accessories - Winter - Medicine, Health Supplys\",\n",
    "    \"Accessories - Winter - Socks\",\n",
    "    \"Accessories - Winter - Technology\",\n",
    "    \"Accessories - Winter - Tuning Supplies\",\n",
    "    \"Bikes\",\n",
    "    \"Bikes - BMX\",\n",
    "    \"Bikes - Comfort\",\n",
    "    \"Bikes - Comfort - Mens\",\n",
    "    \"Bikes - Comfort - Womens\",\n",
    "    \"Bikes - Cruiser\",\n",
    "    \"Bikes - Cruiser - Mens\",\n",
    "    \"Bikes - Cruiser - Womens\",\n",
    "    \"Bikes - Cyclocross\",\n",
    "    \"Bikes - Cyclocross - Mens\",\n",
    "    \"Bikes - Cyclocross - Womens\",\n",
    "    \"Bikes - Dual Sport\",\n",
    "    \"Bikes - Dual Sport - Mens\",\n",
    "    \"Bikes - Dual Sport - Womens\",\n",
    "    \"Bikes - Electric\",\n",
    "    \"Bikes - Electric - Cruiser\",\n",
    "    \"Bikes - Electric - Mens\",\n",
    "    \"Bikes - Electric - Mountain\",\n",
    "    \"Bikes - Electric - Womens\",\n",
    "    \"Bikes - Fatbikes\",\n",
    "    \"Bikes - Fitness\",\n",
    "    \"Bikes - Fitness - Mens\",\n",
    "    \"Bikes - Fitness - Womens\",\n",
    "    \"Bikes - Frames\",\n",
    "    \"Bikes - Gravel & Road\",\n",
    "    \"Bikes - Hybrid\",\n",
    "#     \"Bikes - Hybrid - Mens\",\n",
    "    \"Bikes - Hybrid - Womens\",\n",
    "#     \"Bikes - Kids\",\n",
    "    \"Bikes - Kids - Boys\",\n",
    "    \"Bikes - Kids - Girls\",\n",
    "    \"Bikes - Mountain\",\n",
    "    \"Bikes - Mountain - Full Suspension\",\n",
    "    \"Bikes - Mountain - Full Suspension - Mens\",\n",
    "    \"Bikes - Mountain - Full Suspension - Womens\",\n",
    "    \"Bikes - Mountain - Hardtail\",\n",
    "    \"Bikes - Mountain - Hardtail - Mens\",\n",
    "    \"Bikes - Mountain - Hardtail - Womens\",\n",
    "    \"Bikes - Road - Mens\",\n",
    "    \"Bikes - Road - Womens\",\n",
    "    \"Car Racks - Hitch\",\n",
    "    \"Car Racks - Roof\",\n",
    "    \"Car Racks - Small Parts\",\n",
    "    \"Car Racks - Tailgate Cover\",\n",
    "    \"Car Racks - Trunk\",\n",
    "    \"Clothing\",\n",
    "    \"Clothing - Baselayers\",\n",
    "    \"Clothing - Booties & Shoe Covers\",\n",
    "    \"Clothing - Gloves\",\n",
    "    \"Clothing - Gloves - Kids\",\n",
    "    \"Clothing - Gloves - Mens\",\n",
    "    \"Clothing - Gloves - Womens\",\n",
    "    \"Clothing - Hats & Headwear\",\n",
    "    \"Clothing - Jackets & Vests\",\n",
    "    \"Clothing - Jackets & Vests - Mens\",\n",
    "    \"Clothing - Jackets & Vests - Womens\",\n",
    "    \"Clothing - Jerseys\",\n",
    "    \"Clothing - Jerseys - Mens\",\n",
    "    \"Clothing - Jerseys - Womens\",\n",
    "    \"Clothing - Kids\",\n",
    "    \"Clothing - Pants\",\n",
    "    \"Clothing - Pants - Mens\",\n",
    "    \"Clothing - Pants - Womens\",\n",
    "    \"Clothing - Shirts\",\n",
    "    \"Clothing - Shirts - Mens\",\n",
    "    \"Clothing - Shirts - Womens\",\n",
    "    \"Clothing - Shoes\",\n",
    "    \"Clothing - Shoes - Mountain\",\n",
    "    \"Clothing - Shoes - Mountain - Mens\",\n",
    "    \"Clothing - Shoes - Mountain - Womens\",\n",
    "    \"Clothing - Shoes - Parts\",\n",
    "    \"Clothing - Shoes - Road\",\n",
    "    \"Clothing - Shoes - Road - Mens\",\n",
    "    \"Clothing - Shoes - Road - Womens\",\n",
    "    \"Clothing - Shoes - Street and Casual\",\n",
    "    \"Clothing - Shorts & Bibs\",\n",
    "    \"Clothing - Shorts & Bibs - Mens\",\n",
    "    \"Clothing - Shorts & Bibs - Womens\",\n",
    "    \"Clothing - Socks\",\n",
    "    \"Clothing - Socks - Mens\",\n",
    "    \"Clothing - Socks - Womens\",\n",
    "    \"Clothing - Summer\",\n",
    "    \"Clothing - Summer - Belts\",\n",
    "    \"Clothing - Sweatshirts\",\n",
    "    \"Clothing - Sweatshirts - Mens\",\n",
    "    \"Clothing - Sweatshirts - Womens\",\n",
    "    \"Clothing - Tights & Knickers\",\n",
    "    \"Clothing - Tights & Knickers - Mens\",\n",
    "    \"Clothing - Tights & Knickers - Womens\",\n",
    "    \"Clothing - Warmers\",\n",
    "    \"Clothing - Winter\",\n",
    "    \"Clothing - Winter - Baselayer\",\n",
    "    \"Clothing - Winter - Baselayer - Kids\",\n",
    "    \"Clothing - Winter - Baselayer - Mens\",\n",
    "    \"Clothing - Winter - Baselayer - Womens\",\n",
    "    \"Clothing - Winter - Belts\",\n",
    "    \"Clothing - Winter - Outerwear\",\n",
    "    \"Clothing - Winter - Outerwear - Kids\",\n",
    "    \"Clothing - Winter - Outerwear - Mens\",\n",
    "    \"Clothing - Winter - Outerwear - Mens - Bibs\",\n",
    "    \"Clothing - Winter - Outerwear - Mens - Jackets\",\n",
    "    \"Clothing - Winter - Outerwear - Mens - Pants\",\n",
    "    \"Clothing - Winter - Outerwear - Womens\",\n",
    "    \"Clothing - Winter - Outerwear - Womens - Bibs\",\n",
    "    \"Clothing - Winter - Outerwear - Womens - Jackets\",\n",
    "    \"Clothing - Winter - Outerwear - Womens - Pants\",\n",
    "    \"Clothing - Winter - Shoes / Boots\",\n",
    "    \"Clothing - Winter - Street Wear\",\n",
    "    \"Clothing - Winter - Street Wear - Kids\",\n",
    "    \"Clothing - Winter - Street Wear - Mens\",\n",
    "    \"Clothing - Winter - Street Wear - Unisex\",\n",
    "    \"Clothing - Winter - Street Wear - Womens\",\n",
    "    \"Demo Sales\",\n",
    "    \"Event Tickets\",\n",
    "    \"Face Mask\",\n",
    "    \"Labor\",\n",
    "    \"Lift Tickets\",\n",
    "    \"Logo Merchandise\",\n",
    "    \"Logo Merchandise - Clothing\",\n",
    "#     \"Logo Merchandise - Clothing - Buffs/Facemasks\",\n",
    "    \"Logo Merchandise - Clothing - Crewneck Sweatshirts\",\n",
    "    \"Logo Merchandise - Clothing - Hats\",\n",
    "    \"Logo Merchandise - Clothing - Hats - Beanies\",\n",
    "    \"Logo Merchandise - Clothing - Hats - Caps\",\n",
    "    \"Logo Merchandise - Clothing - Jerseys\",\n",
    "    \"Logo Merchandise - Clothing - Long Sleeve Shirts\",\n",
    "    \"Logo Merchandise - Clothing - Outer Wear\",\n",
    "    \"Logo Merchandise - Clothing - Performance/Tech Tee\",\n",
    "    \"Logo Merchandise - Clothing - Pullover Hoodies\",\n",
    "    \"Logo Merchandise - Clothing - Short Sleeve T-Shirt\",\n",
    "    \"Logo Merchandise - Clothing - Socks\",\n",
    "    \"Logo Merchandise - Clothing - Street Wear\",\n",
    "    \"Logo Merchandise - Houseware\",\n",
    "    \"Logo Merchandise - Impulse\",\n",
    "    \"Logo Merchandise - Waterbottles\",\n",
    "    \"Parts\",\n",
    "    \"Parts - Bar Ends & Aerobars\",\n",
    "    \"Parts - Bearings\",\n",
    "    \"Parts - Bottom Brackets\",\n",
    "    \"Parts - Brakes\",\n",
    "    \"Parts - Brakes - BrakeSet\",\n",
    "    \"Parts - Brakes - Disc\",\n",
    "    \"Parts - Brakes - Levers\",\n",
    "    \"Parts - Brakes - Pads\",\n",
    "    \"Parts - Cables & Housing\",\n",
    "    \"Parts - Cassettes & Freewheels\",\n",
    "    \"Parts - Chains\",\n",
    "    \"Parts - Cranks & Chainrings\",\n",
    "    \"Parts - Decals & Paint\",\n",
    "    \"Parts - Derailleurs\",\n",
    "    \"Parts - EBIKE Parts\",\n",
    "    \"Parts - Fenders\",\n",
    "    \"Parts - Forks\",\n",
    "    \"Parts - Forks - Rigid\",\n",
    "    \"Parts - Forks - Suspension\",\n",
    "    \"Parts - Frame parts\",\n",
    "    \"Parts - Grips and Tape\",\n",
    "    \"Parts - Grips and Tape - Grips\",\n",
    "    \"Parts - Grips and Tape - Handlebar Tape\",\n",
    "    \"Parts - Handlebars\",\n",
    "    \"Parts - Headsets\",\n",
    "    \"Parts - Hubs & Skewers\",\n",
    "    \"Parts - Pedals\",\n",
    "    \"Parts - Rims\",\n",
    "    \"Parts - Saddles\",\n",
    "    \"Parts - Screws, Bolts and Fasteners\",\n",
    "    \"Parts - Seatposts\",\n",
    "    \"Parts - Shifters\",\n",
    "    \"Parts - Spokes\",\n",
    "    \"Parts - Stems\",\n",
    "    \"Parts - Suspension\",\n",
    "    \"Parts - Tires\",\n",
    "    \"Parts - Tools\",\n",
    "    \"Parts - Tubeless Accessories\",\n",
    "    \"Parts - Tubes\",\n",
    "    \"Parts - Wheels\",\n",
    "    \"Parts - Wheels - Accessories\",\n",
    "    \"Parts - Wheels - Front\",\n",
    "    \"Parts - Wheels - Rear\",\n",
    "    \"Parts - Wheels - Sets\",\n",
    "#     \"Parts - Winter\",\n",
    "    \"Rental - Bike - Electric\",\n",
    "    \"Rental - Bike - Mountain\",\n",
    "    \"Rental - Bike - Road\",\n",
    "    \"Rental - Bike - Standard\",\n",
    "    \"Rental - Helmet\",\n",
    "    \"Rental - Winter Daily - Poles\",\n",
    "    \"Rental - Winter Daily - Skis\",\n",
    "    \"Rental - Winter Daily - Snowboard\",\n",
    "    \"Rental - Winter Daily - Snowshoe\",\n",
    "    \"Rental - Winter Daily - Nordic\",\n",
    "    \"Rental - Winter Season - Skis\",\n",
    "    \"Rental - Winter Season - Snowboard\",\n",
    "    \"Reservations\",\n",
    "    \"Season Pass\",\n",
    "    \"Services\",\n",
    "    \"Services - Fits\",\n",
    "    \"Services - Shipping & Handling\",\n",
    "    \"Ski Hardgoods\",\n",
    "    \"Ski Hardgoods - Accessories\",\n",
    "    \"Ski Hardgoods - Bindings\",\n",
    "    \"Ski Hardgoods - Bindings - Alpine\",\n",
    "    \"Ski Hardgoods - Bindings - AT\",\n",
    "    \"Ski Hardgoods - Boot Parts/Insoles\",\n",
    "    \"Ski Hardgoods - Boots\",\n",
    "    \"Ski Hardgoods - Boots - Junior\",\n",
    "    \"Ski Hardgoods - Boots - Men\",\n",
    "    \"Ski Hardgoods - Boots - Women\",\n",
    "    \"Ski Hardgoods - Parts\",\n",
    "    \"Ski Hardgoods - Poles\",\n",
    "    \"Ski Hardgoods - Poles - Kids\",\n",
    "    \"Ski Hardgoods - Skis\",\n",
    "    \"Ski Hardgoods - Skis - Junior\",\n",
    "    \"Ski Hardgoods - Skis - Men\",\n",
    "    \"Ski Hardgoods - Skis - System\",\n",
    "    \"Ski Hardgoods - Skis - Women\",\n",
    "    \"Ski School\",\n",
    "    \"Snowboard Hardgoods\",\n",
    "    \"Snowboard Hardgoods - Accessories/Parts\",\n",
    "    \"Snowboard Hardgoods - Bindings\",\n",
    "    \"Snowboard Hardgoods - Bindings - Junior\",\n",
    "    \"Snowboard Hardgoods - Bindings - Men\",\n",
    "    \"Snowboard Hardgoods - Bindings - Women\",\n",
    "    \"Snowboard Hardgoods - Boards\",\n",
    "    \"Snowboard Hardgoods - Boards - Junior\",\n",
    "    \"Snowboard Hardgoods - Boards - Men\",\n",
    "    \"Snowboard Hardgoods - Boards - Women\",\n",
    "    \"Snowboard Hardgoods - Boots\",\n",
    "    \"Snowboard Hardgoods - Boots - Junior\",\n",
    "    \"Snowboard Hardgoods - Boots - Men\",\n",
    "    \"Snowboard Hardgoods - Boots - Women\",\n",
    "    \"Snowboard Hardgoods - Poles\",\n",
    "    \"Snowshoe\",\n",
    "    \"Summer Demo\",\n",
    "    \"Sunlight Gift Card\",\n",
    "    \"Winter Demo\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5e8cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════════╤════════════╤═════════════╤═══════════╕\n",
      "│ Model                  │   Accuracy │   Precision │ Success   │\n",
      "╞════════════════════════╪════════════╪═════════════╪═══════════╡\n",
      "│ Logistic Regression    │       0.99 │        0.99 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Decision Tree          │       1    │        1    │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Naive Bayes            │       0.93 │        0.94 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Random Forest          │       0.99 │        1    │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Support Vector Machine │       0.98 │        0.99 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ XGBoost                │       0.98 │        1    │ Success   │\n",
      "╘════════════════════════╧════════════╧═════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = products[products['Category'].isin(categories)]\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Define a CountVectorizer instance with the list of stopwords for BoW\n",
    "vectorizer_bow = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define a TfidfVectorizer instance with the list of stopwords for TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Text preprocessing for 'Description' using BoW\n",
    "description_bow = vectorizer_bow.fit_transform(data['Description'])\n",
    "\n",
    "# Text preprocessing for 'Description' using TF-IDF\n",
    "description_tfidf = vectorizer_tfidf.fit_transform(data['Description'])\n",
    "\n",
    "# Encode categorical features 'Brand' and 'Category'\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "encoded_brand = encoder.fit_transform(data[['Brand']])\n",
    "encoded_category = encoder.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features for BoW\n",
    "combined_features_bow = np.hstack((description_bow.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Combine features for TF-IDF\n",
    "combined_features_tfidf = np.hstack((description_tfidf.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(combined_features_bow, data['Category'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(combined_features_tfidf, data['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(),\n",
    "}\n",
    "\n",
    "# Define an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"XGBoost\":\n",
    "        # Train XGBoost model\n",
    "        model.fit(X_train_bow, y_train_encoded)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred_encoded = model.predict(X_test_bow)\n",
    "    else:\n",
    "        # Train other models\n",
    "        if \"Naive Bayes\" in name:\n",
    "            model.fit(X_train_tfidf, y_train_encoded)\n",
    "            y_pred_encoded = model.predict(X_test_tfidf)\n",
    "        else:\n",
    "            model.fit(X_train_bow, y_train_encoded)\n",
    "            y_pred_encoded = model.predict(X_test_bow)\n",
    "\n",
    "    # Decode the predicted labels back to original category names\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    # Append results to the list\n",
    "    results.append([name, f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"])\n",
    "\n",
    "\n",
    "# Print table\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6308e232",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "Doc2Vec is a modeling technique developed as an extension of Word2Vec, which is widely used for natural language processing (NLP) tasks. While Word2Vec learns continuous vector representations (word embeddings) for words in a text corpus, Doc2Vec extends this idea to learn embeddings for entire documents or sentences.\n",
    "\n",
    "In Doc2Vec, each document is represented as a fixed-length vector, similar to how words are represented in Word2Vec. This vector representation captures the semantic meaning of the document in a continuous vector space. The model is trained in an unsupervised manner, where it learns to predict the context of words or documents within a given window size. \n",
    "\n",
    "There are two main architectures for Doc2Vec:\n",
    "\n",
    "1. **Distributed Memory (DM)**: In this architecture, the model learns to predict the next word in a context given a fixed-length vector representation of the document and the context words. This architecture is similar to the Skip-gram model in Word2Vec.\n",
    "\n",
    "2. **Distributed Bag of Words (DBOW)**: In this architecture, the model learns to predict words in a context given only the fixed-length vector representation of the document. The document vector is randomly sampled from the document during training, and the model learns to predict words in the context of this vector. This architecture is similar to the Continuous Bag of Words (CBOW) model in Word2Vec.\n",
    "\n",
    "Doc2Vec models have been widely used for various NLP tasks, including document classification, sentiment analysis, information retrieval, and clustering. They provide a way to represent documents in a continuous vector space, enabling efficient similarity calculations and downstream tasks.\n",
    "Doc2Vec object does not have a fit attribute. This is because Doc2Vec does not follow the same fitting process as traditional machine learning models like logistic regression or decision trees.\n",
    "\n",
    "With Doc2Vec, you typically build the vocabulary and train the model separately from the fit and predict workflow used in supervised learning models. You first build the vocabulary using the build_vocab method and then train the model using the train method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca5de996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/katrinaengelsted/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/katrinaengelsted/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════╤════════════╤═════════════╤═══════════╕\n",
      "│ Model   │   Accuracy │   Precision │ Success   │\n",
      "╞═════════╪════════════╪═════════════╪═══════════╡\n",
      "│ Doc2Vec │       0.51 │        0.66 │ Success   │\n",
      "╘═════════╧════════════╧═════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [token for token in tokens if token not in string.punctuation and token not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and preprocess descriptions\n",
    "products['Processed_Description'] = products['Description'].apply(preprocess_text)\n",
    "\n",
    "# Convert descriptions to TaggedDocument objects\n",
    "tagged_data = [TaggedDocument(words=doc, tags=[i]) for i, doc in enumerate(products['Processed_Description'])]\n",
    "\n",
    "# Train Doc2Vec model\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "products['Category_Encoded'] = label_encoder.fit_transform(products['Category'])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(products['Processed_Description'], products['Category_Encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Infer vectors for descriptions\n",
    "X_train_vectors = [model.infer_vector(doc) for doc in X_train]\n",
    "X_test_vectors = [model.infer_vector(doc) for doc in X_test]\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Predict categories for test data\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Add the predicted categories to the DataFrame\n",
    "products.loc[X_test.index, 'Predicted_Category'] = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "results = [[\"Doc2Vec\", f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"]]\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8fe471",
   "metadata": {},
   "source": [
    "Doc2Vec may perform poorly on short product descriptions for several reasons:\n",
    "\n",
    "Lack of Sufficient Context: Doc2Vec relies on learning vector representations for documents based on their surrounding context. Short product descriptions may not provide enough context for the model to capture the semantic meaning accurately. As a result, the learned document embeddings may not effectively represent the underlying information in the descriptions.\n",
    "\n",
    "Limited Vocabulary: Short descriptions may contain a limited vocabulary, leading to fewer training examples for the model to learn from. Doc2Vec models benefit from a diverse range of words and phrases to build robust document embeddings. If the vocabulary in the descriptions is limited, the model may struggle to generalize well to unseen data.\n",
    "\n",
    "Semantic Variability: Short descriptions may contain ambiguous or vague language, making it challenging for the model to infer the correct category. Since Doc2Vec learns continuous vector representations based on the context of words, it may struggle with capturing nuanced semantic meanings from short and ambiguous text.\n",
    "\n",
    "Data Sparsity: Short descriptions may result in sparse document representations, especially if they contain only a few words or phrases. Sparse representations may not adequately capture the information needed for accurate categorization, leading to suboptimal performance.\n",
    "\n",
    "To address these issues, it's essential to consider alternative approaches such as incorporating additional features, using different pre-trained embeddings, or leveraging domain-specific knowledge to enhance the performance of the classification task for short product descriptions. Additionally, experimenting with different model architectures and hyperparameters may also help improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# products.dropna(subset=['Category'])\n",
    "products['Category'] = products['Category'].astype(str)\n",
    "\n",
    "# Create an empty list to store the parent categories\n",
    "parent_categories = []\n",
    "for category in products['Category']:\n",
    "    parts = category.split(\"-\", 1)\n",
    "    parent_category = parts[0] if len(parts) > 1 else category  # Use original if no \"-\"\n",
    "    if parent_category == 'X':\n",
    "        parent_category = 'Cross Country'\n",
    "    parent_categories.append(parent_category)\n",
    "    \n",
    "\n",
    "# Add the new parent category to the DataFrame\n",
    "products['ParentCategory'] = parent_categories\n",
    "\n",
    "print(products['ParentCategory'].nunique, products['Category'].nunique)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e80996",
   "metadata": {},
   "source": [
    "## FastText\n",
    "\n",
    "FastText is a library developed by Facebook AI Research for efficient learning of word representations and sentence classification. It's an extension of the Word2Vec model that also considers subword information. Here's how FastText works for classification:\n",
    "\n",
    "* Word Representations: FastText learns continuous representations for words in a text corpus. It represents each word as a vector in a high-dimensional space, capturing semantic and syntactic information.\n",
    "\n",
    "* Subword Information: Unlike traditional word embeddings, FastText also considers subword information by breaking words into smaller n-grams (character sequences). This allows FastText to handle out-of-vocabulary words and capture morphological similarities.\n",
    "\n",
    "* Text Classification: FastText can be used for text classification tasks, such as categorizing product descriptions into predefined categories. It learns a classifier based on the word embeddings and predicts the most likely category for a given input text.\n",
    "\n",
    "To train a FastText model for classification, you can follow these steps:\n",
    "\n",
    "* Prepare Data: Organize your product descriptions and corresponding categories into a suitable format for training. Each training example should consist of a text description and its corresponding category label.\n",
    "\n",
    "* Preprocess Text: Clean and preprocess the text data, including tokenization, lowercasing, and removing stopwords or special characters.\n",
    "\n",
    "* Training: Train the FastText model using the preprocessed text data. You can specify parameters such as the dimensionality of word vectors, the context window size, and the number of training epochs.\n",
    "\n",
    "* Evaluation: Evaluate the trained model using a separate validation dataset to assess its performance in categorizing product descriptions accurately. You can calculate metrics such as accuracy, precision, recall, and F1-score.\n",
    "\n",
    "* Prediction: Once the model is trained and evaluated, you can use it to predict the category labels for new product descriptions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71e37ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/katrinaengelsted/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/katrinaengelsted/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/katrinaengelsted/nltk_data...\n",
      "/var/folders/vx/n4xm54gd5g15czs1d5cvl6_m0000gn/T/ipykernel_99248/3939672140.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Processed_Description'] = data['Description'].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing and removing punctuation\n",
    "    tokens = [token.lower() for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the \"Description\" field\n",
    "data['Processed_Description'] = data['Description'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c6a131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════════════════════════╤════════════╤═════════════╤═══════════╕\n",
      "│ Model                  │   Accuracy │   Precision │ Success   │\n",
      "╞════════════════════════╪════════════╪═════════════╪═══════════╡\n",
      "│ Logistic Regression    │       0.99 │        0.99 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Decision Tree          │       1    │        1    │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Naive Bayes            │       0.93 │        0.94 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Random Forest          │       0.99 │        1    │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Support Vector Machine │       0.98 │        0.99 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ XGBoost                │       0.98 │        0.99 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ Doc2Vec                │       0.51 │        0.66 │ Success   │\n",
      "├────────────────────────┼────────────┼─────────────┼───────────┤\n",
      "│ FastText               │       0.76 │        0.78 │ Success   │\n",
      "╘════════════════════════╧════════════╧═════════════╧═══════════╛\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "model.build_vocab(corpus_iterable=data['Processed_Description'])\n",
    "model.train(corpus_iterable=data['Processed_Description'], total_examples=len(data['Processed_Description']), epochs=model.epochs)\n",
    "\n",
    "# Convert descriptions to average word vectors\n",
    "X_fasttext = np.array([np.mean([model.wv[word] for word in words if word in model.wv] or [np.zeros(100)], axis=0) for words in data['Processed_Description']])\n",
    "\n",
    "# Encode the target variable 'Category' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(data['Category'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fasttext, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict categories for test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "# Add results for FastText model\n",
    "results.append([\"FastText\", f\"{accuracy:.2f}\", f\"{precision:.2f}\", f\"{'Success' if accuracy > 0.5 else 'Failure'}\"])\n",
    "\n",
    "# Print updated table\n",
    "print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Success\"], tablefmt=\"fancy_grid\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d018892",
   "metadata": {},
   "source": [
    "FastText might not have performed as well as some other models in this classification task for a few reasons:\n",
    "\n",
    "Complexity of the Task: FastText is effective for capturing semantic meaning and relationships between words, especially in longer texts. However, if the product descriptions are short and lack enough context, FastText may not be able to capture sufficient semantic information to accurately classify the categories.\n",
    "\n",
    "* Data Size: FastText typically performs better with larger amounts of text data. If the dataset used for training is relatively small, FastText may not have had enough examples to learn robust representations of the product categories.\n",
    "\n",
    "* Quality of Preprocessing: The preprocessing steps applied to the product descriptions could impact FastText's performance. If the preprocessing steps removed important information or introduced noise into the data, it could have affected the model's ability to learn meaningful representations.\n",
    "\n",
    "* Parameter Tuning: FastText has several hyperparameters that can influence its performance, such as vector size, window size, and epochs. If these hyperparameters were not tuned effectively for the specific task and dataset, the model's performance could suffer.\n",
    "\n",
    "* Class Imbalance: If the distribution of categories in the dataset is imbalanced, meaning some categories have significantly fewer examples than others, FastText may struggle to accurately classify the minority classes.\n",
    "\n",
    "Overall, while FastText is a powerful tool for natural language processing tasks, its performance can vary depending on the specific characteristics of the dataset and the task at hand. In the case of short product descriptions and category classification, other models like logistic regression, decision trees, and random forests might outperform FastText due to their simplicity and ability to capture patterns in smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to combined features\n",
    "pca = PCA(n_components=2)  # You can choose 2 or 3 components for 2D or 3D visualization\n",
    "combined_features_pca = pca.fit_transform(combined_features_bow)\n",
    "\n",
    "# Assuming y_pred_encoded and combined_features_pca are your arrays\n",
    "print(\"Shape of y_pred_encoded:\", y_pred_encoded.shape)\n",
    "print(\"Shape of combined_features_pca:\", combined_features_pca.shape)\n",
    "\n",
    "if y_pred_encoded.shape[0] != combined_features_pca.shape[0]:\n",
    "    # Reshape y_pred_encoded to match the number of rows in combined_features_pca\n",
    "    y_pred_encoded = np.resize(y_pred_encoded, (combined_features_pca.shape[0],))\n",
    "\n",
    "# Now, both arrays should have the same number of rows\n",
    "print(\"Shape of y_pred_encoded after adjustment:\", y_pred_encoded.shape)\n",
    "\n",
    "# # Plot the transformed features\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(combined_features_pca[:, 0], combined_features_pca[:, 1], c=y_pred_encoded, cmap='viridis', alpha=0.5)\n",
    "plt.title('PCA Visualization of Model Predictions')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(label='Predicted Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a5281",
   "metadata": {},
   "source": [
    "The data visualization shows a two-dimensional principal component analysis (PCA) plot where the  x-axis is labeled \"Principal Component 1\" and the y-axis is labeled \"Principal Component 2\". The text at the top of the plot says \"PCA Visualization of Model Predictions\".  There are also labels for \"Predicted Category\"  along the color bar on the right.  Unfortunately, it is difficult to draw specific conclusions from this visualization because the  number of data points and the color scale are not provided.\n",
    "\n",
    "Here are some general observations about PCA visualizations:\n",
    "\n",
    "PCA is a dimensionality reduction technique that is often used to visualize high-dimensional data. It reduces the number of dimensions by finding new components, called principal components, that capture the most variance in the data.\n",
    "In a PCA visualization, each data point is projected onto the new principal components. The resulting plot can be used to see how the data points are clustered or grouped together.\n",
    "In the context of product categorization, a PCA visualization could be used to see how well a model is able to categorize products into different categories. The data points would represent the products, and the color would represent the predicted category. If the model is doing a good job of categorization, the data points for each category would be clustered together in the PCA plot.\n",
    "\n",
    "However, it is important to keep in mind that PCA visualizations can be difficult to interpret, especially for high-dimensional data.  If you are interested in learning more about PCA visualizations, you can search for  \"PCA visualization interpretation: https://innovationyourself.com/master-machine-learning-with-pca/\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf05a7",
   "metadata": {},
   "source": [
    "## Exporting Results\n",
    "\n",
    "To get a tangible sense of what these models are doing, let's export a file to see the actual versus predicted values. This will also give us insight into which categories are difficult to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab68f5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options for pandas DataFrame\n",
    "pd.set_option('display.max_colwidth', None)  # Display full width of columns\n",
    "pd.set_option('display.max_rows', None)      # Display all rows\n",
    "\n",
    "# Create a DataFrame to display the test data output with Description and Category\n",
    "test_output = pd.DataFrame({\n",
    "    'Description': data.loc[y_test.index, 'Description'].values,\n",
    "    'Actual_Category': label_encoder.inverse_transform(y_test_encoded),\n",
    "    'Predicted_Category': label_encoder.inverse_transform(y_pred_encoded)\n",
    "})\n",
    "\n",
    "# Export the test data output to a CSV file\n",
    "test_output.to_csv('../data/processed/full_output.csv', index=False)\n",
    "\n",
    "# Print a message to confirm the export\n",
    "print(\"Test data output exported to '../data/processed/full_output.csv' successfully.\")\n",
    "\n",
    "# Reset display options to default after printing\n",
    "pd.reset_option('display.max_colwidth')\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1f00c",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ce9cd",
   "metadata": {},
   "source": [
    "Summary of Product Category Classification Results:\n",
    "\n",
    "* Logistic Regression: Achieved an accuracy of 0.99 and precision of 0.99. Logistic Regression is a linear model that works well for binary classification tasks. It performed exceptionally well in this scenario, likely due to its simplicity and ability to capture linear relationships between features and target classes.\n",
    "\n",
    "* Decision Tree: Also achieved high accuracy and precision of 0.99 and 1, respectively. Decision Trees are known for their interpretability and ability to handle non-linear relationships in the data. In this case, the Decision Tree model effectively learned complex decision boundaries, leading to accurate predictions.\n",
    "\n",
    "* Naive Bayes: Achieved an accuracy of 0.93 and precision of 0.94. Naive Bayes classifiers are based on the assumption of independence between features, which might not hold true for text data with short descriptions. Despite this limitation, Naive Bayes performed reasonably well but may struggle with capturing subtle dependencies between words in the descriptions.\n",
    "\n",
    "* Random Forest: Achieved high accuracy of 0.99 and precision of 1. Random Forest is an ensemble learning method that combines multiple decision trees to improve performance. It excelled in capturing complex relationships in the data and mitigating overfitting, leading to high accuracy and precision.\n",
    "\n",
    "* Support Vector Machine (SVM): Achieved an accuracy of 0.98 and precision of 0.99. SVMs are effective for high-dimensional data and can learn complex decision boundaries. The slightly lower accuracy compared to other models could be due to the choice of kernel and parameters, which may need further tuning for optimal performance.\n",
    "\n",
    "* XGBoost: Achieved an accuracy of 0.98 and precision of 0.99. XGBoost is a powerful gradient boosting algorithm that builds a sequence of decision trees to improve predictive performance. It performed well in this scenario but may require more computational resources and tuning compared to other models.\n",
    "\n",
    "**Factors Influencing Accuracy and Precision:**\n",
    "\n",
    "The accuracy and precision of the models can vary based on several factors, including:\n",
    "\n",
    "* Short Descriptions: Short product descriptions may contain limited information, making it challenging for models to accurately classify categories. Models that can effectively capture the essence of the descriptions, such as Decision Trees and Random Forests, may perform better in this scenario.\n",
    "\n",
    "* Category Complexity: Some categories may be inherently more challenging to classify due to ambiguity or similarity with other categories. Models with the ability to capture subtle differences between categories, such as Logistic Regression and Support Vector Machines, may excel in such cases.\n",
    "\n",
    "* Feature Representation: The choice of feature representation, such as Bag-of-Words or TF-IDF, can impact model performance. Models like Naive Bayes, which rely on feature independence assumptions, may struggle with capturing the semantic meaning of words in short descriptions compared to more advanced models like Word Embeddings (e.g., FastText and Doc2Vec).\n",
    "\n",
    "Overall, while all models achieved high accuracy and precision, the choice of model depends on the specific characteristics of the dataset and the desired balance between interpretability and predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989b562",
   "metadata": {},
   "source": [
    "Additional Resources: \n",
    "- [blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)\n",
    "- [textcategorizer](https://spacy.io/api/textcategorizer)\n",
    "- [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
    "- [vec regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "df_products_clean = df_products.dropna()\n",
    "\n",
    "# Plot count of products in each parent category\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_products, x='ParentCategory')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Parent Category')\n",
    "plt.ylabel('Count of Products')\n",
    "plt.title('Number of Products in Each Parent Category')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
