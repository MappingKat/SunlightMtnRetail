{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721ecc23",
   "metadata": {},
   "source": [
    "# 4. Data Modeling\n",
    "* [4 Training Data](#2_Data_training_introduction)\n",
    "  * [4.1 Dummy Variables/One Hot Encoding for Categorical](#3.1_one_hot_encoding)\n",
    "  * [4.2 Standardize Numerical Data](#3.2_standardize)\n",
    "  * [4.3 Testing Training](#3.3_testing_training)\n",
    " * [4.2 Summary](#3.7_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a48204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.executable\n",
    "\n",
    "# !/opt/homebrew/anaconda3/bin/python -m pip install xgboost fasttext nltk spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e8745e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2661 entries, 9454 to 4138\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   Brand           2661 non-null   category\n",
      " 1   Description     2661 non-null   string  \n",
      " 2   Keyword         2661 non-null   object  \n",
      " 3   UPC             2661 non-null   object  \n",
      " 4   MSRP            2661 non-null   float64 \n",
      " 5   Quantity        2661 non-null   int64   \n",
      " 6   SKU             2661 non-null   object  \n",
      " 7   Color           2661 non-null   string  \n",
      " 8   Size            2661 non-null   string  \n",
      " 9   StyleNumber     2661 non-null   object  \n",
      " 10  StyleName       2661 non-null   object  \n",
      " 11  ParentCategory  2661 non-null   string  \n",
      "dtypes: category(1), float64(1), int64(1), object(5), string(4)\n",
      "memory usage: 259.3+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "%store -r products\n",
    "%store -r X_train\n",
    "%store -r X_test\n",
    "%store -r y_train\n",
    "%store -r y_test\n",
    "#print(products) \n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cec2e4",
   "metadata": {},
   "source": [
    "Resources: \n",
    "- [blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)\n",
    "- [textcategorizer](https://spacy.io/api/textcategorizer)\n",
    "- [doc2vec](https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html#sphx-glr-auto-examples-tutorials-run-doc2vec-lee-py)\n",
    "- [vec regression](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b6c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "# Example creation of a Doc2Vec model instance\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "doc2vec_model = Doc2Vec(vector_size=100, min_count=2, epochs=40)\n",
    "\n",
    "tokenized_descriptions = [description.split() for description in products['Description']]\n",
    "\n",
    "# Convert each document (product description) to a TaggedDocument object\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(tokenized_descriptions)]\n",
    "\n",
    "# Now tagged_data contains a list of TaggedDocument objects\n",
    "\n",
    "# Build vocabulary from the tagged data\n",
    "doc2vec_model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "doc2vec_model.train(tagged_data, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f12db1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.99\n",
      "Precision: 0.98\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 0  5  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree\n",
      "Accuracy: 0.99\n",
      "Precision: 0.99\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 0  5  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.92\n",
      "Precision: 0.88\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.99\n",
      "Precision: 0.98\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 0  5  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.98\n",
      "Precision: 0.97\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 0  5  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoc2Vec\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;66;03m# Train Doc2Vec model\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m         model\u001b[38;5;241m.\u001b[39mbuild_vocab(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     68\u001b[0m         X_train_doc2vec \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39minfer_vector(doc\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data))]]\n\u001b[1;32m     69\u001b[0m         X_test_doc2vec \u001b[38;5;241m=\u001b[39m [model\u001b[38;5;241m.\u001b[39minfer_vector(doc\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDescription\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)):]]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/gensim/models/doc2vec.py:882\u001b[0m, in \u001b[0;36mDoc2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_vocab\u001b[39m(\n\u001b[1;32m    842\u001b[0m         \u001b[38;5;28mself\u001b[39m, corpus_iterable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, corpus_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, progress_per\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m    843\u001b[0m         keep_raw_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, trim_rule\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    844\u001b[0m     ):\n\u001b[1;32m    845\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    846\u001b[0m \n\u001b[1;32m    847\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    880\u001b[0m \n\u001b[1;32m    881\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m     total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_vocab(\n\u001b[1;32m    883\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file,\n\u001b[1;32m    884\u001b[0m         progress_per\u001b[38;5;241m=\u001b[39mprogress_per, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count \u001b[38;5;241m=\u001b[39m corpus_count\n\u001b[1;32m    887\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words \u001b[38;5;241m=\u001b[39m total_words\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/gensim/models/doc2vec.py:1054\u001b[0m, in \u001b[0;36mDoc2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1052\u001b[0m     corpus_iterable \u001b[38;5;241m=\u001b[39m TaggedLineDocument(corpus_file)\n\u001b[0;32m-> 1054\u001b[0m total_words, corpus_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[1;32m   1056\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollected \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m word types and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m unique tags from a corpus of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m examples and \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m words\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_vocab), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdv), corpus_count, total_words,\n\u001b[1;32m   1059\u001b[0m )\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/gensim/models/doc2vec.py:956\u001b[0m, in \u001b[0;36mDoc2Vec._scan_vocab\u001b[0;34m(self, corpus_iterable, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m document_no, document \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(corpus_iterable):\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checked_string_types:\n\u001b[0;32m--> 956\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document\u001b[38;5;241m.\u001b[39mwords, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    957\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    958\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be a list of words (usually unicode strings). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    959\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFirst \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m here is instead plain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    960\u001b[0m                 \u001b[38;5;28mtype\u001b[39m(document\u001b[38;5;241m.\u001b[39mwords),\n\u001b[1;32m    961\u001b[0m             )\n\u001b[1;32m    962\u001b[0m         checked_string_types \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load data\n",
    "data = products\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Define a CountVectorizer instance with the list of stopwords for BoW\n",
    "vectorizer_bow = CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define a TfidfVectorizer instance with the list of stopwords for TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Define an OneHotEncoder instance for encoding categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Text preprocessing for 'Description' using BoW\n",
    "description_bow = vectorizer_bow.fit_transform(data['Description'])\n",
    "\n",
    "# Text preprocessing for 'Description' using TF-IDF\n",
    "description_tfidf = vectorizer_tfidf.fit_transform(data['Description'])\n",
    "\n",
    "# Encode categorical features 'Brand' and 'Category'\n",
    "encoded_brand = encoder.fit_transform(data[['Brand']])\n",
    "encoded_category = encoder.fit_transform(data[['Category']])\n",
    "\n",
    "# Combine features for BoW\n",
    "combined_features_bow = np.hstack((description_bow.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Combine features for TF-IDF\n",
    "combined_features_tfidf = np.hstack((description_tfidf.toarray(), encoded_brand, encoded_category))\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(combined_features_bow, data['Category'], test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(combined_features_tfidf, data['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Doc2Vec\": gensim.models.Doc2Vec(vector_size=100, min_count=2, epochs=40),\n",
    "    # \"FastText\": fasttext.train_supervised(input=description_tfidf, epoch=25, wordNgrams=2),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if name == \"Doc2Vec\":\n",
    "        # Train Doc2Vec model\n",
    "        model.build_vocab(data['Description'])\n",
    "        X_train_doc2vec = [model.infer_vector(doc.split()) for doc in data['Description'][:int(0.8 * len(data))]]\n",
    "        X_test_doc2vec = [model.infer_vector(doc.split()) for doc in data['Description'][int(0.8 * len(data)):]]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train_doc2vec, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test_doc2vec)\n",
    "    # elif name == \"FastText\":\n",
    "    #     # Train FastText model\n",
    "    #     model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    #     # Make predictions\n",
    "    #     y_pred = model.predict(X_test_tfidf)\n",
    "    else:\n",
    "        # Train other models\n",
    "        if \"Naive Bayes\" in name:\n",
    "            model.fit(X_train_tfidf, y_train)\n",
    "            y_pred = model.predict(X_test_tfidf)\n",
    "        else:\n",
    "            model.fit(X_train_bow, y_train)\n",
    "            y_pred = model.predict(X_test_bow)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8e47fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy\n",
    "print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddeb3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression\n",
      "Accuracy: 0.93\n",
      "Precision: 0.92\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 2  3  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 16  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "Model: Decision Tree\n",
      "Accuracy: 0.92\n",
      "Precision: 0.93\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0 ...  0  0  0]\n",
      " [ 2  3  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 16  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "Model: Naive Bayes\n",
      "Accuracy: 0.75\n",
      "Precision: 0.72\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  1 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 16  0  0]\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest\n",
      "Accuracy: 0.95\n",
      "Precision: 0.95\n",
      "Confusion Matrix:\n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 2  3  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Accuracy: 0.92\n",
      "Precision: 0.90\n",
      "Confusion Matrix:\n",
      "[[ 1  0  0 ...  0  0  0]\n",
      " [ 2  3  0 ...  0  0  0]\n",
      " [ 0  0 10 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 16  0  0]\n",
      " [ 0  0  0 ...  0  4  0]\n",
      " [ 0  0  0 ...  0  0  1]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = products\n",
    "\n",
    "# Define a list of stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Bag of Words (BoW) method\n",
    "vectorizer_bow = CountVectorizer(stop_words=stop_words)\n",
    "description_bow = vectorizer_bow.fit_transform(data['Description'])\n",
    "\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) method\n",
    "vectorizer_tfidf = TfidfVectorizer(stop_words=stop_words)\n",
    "description_tfidf = vectorizer_tfidf.fit_transform(data['Description'])\n",
    "\n",
    "# Word Embeddings (Word2Vec) method\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=data['Description'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word_embeddings = []\n",
    "for word in data['Description']:\n",
    "    if word in word2vec_model.wv:\n",
    "        word_embeddings.append(word2vec_model.wv[word])\n",
    "    else:\n",
    "        word_embeddings.append(np.zeros(100))  # If word not present, use zero vector\n",
    "\n",
    "# Doc2Vec method\n",
    "# doc2vec_model = gensim.models.Doc2Vec(vector_size=100, min_count=2, epochs=40)\n",
    "# doc2vec_model.build_vocab(data['Description'])\n",
    "# doc2vec_model.train(data['Description'], total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "\n",
    "# TextCategorizer (spaCy) method\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the text categorizer component to the pipeline using its string name\n",
    "nlp.add_pipe('textcat')\n",
    "\n",
    "# Machine Learning Classification Algorithms\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Description'], data['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Support Vector Machine\": SVC()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    if \"Naive Bayes\" in name:\n",
    "        model.fit(vectorizer_tfidf.transform(X_train), y_train)\n",
    "        y_pred = model.predict(vectorizer_tfidf.transform(X_test))\n",
    "    else:\n",
    "        model.fit(vectorizer_bow.transform(X_train), y_train)\n",
    "        y_pred = model.predict(vectorizer_bow.transform(X_test))\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Calculate precision\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(f\"Confusion Matrix:\\n{cm}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
