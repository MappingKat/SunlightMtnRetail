{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41896ce4",
   "metadata": {},
   "source": [
    "# 3. Pre-processing and Training Data Development\n",
    "\n",
    "* [3 Training Data](#2_Data_training_introduction)\n",
    "  * [3.1 Dummy Variables/One Hot Encoding for Categorical](#2.1_one_hot_encoding)\n",
    "  * [3.2 Standardize Numerical Data](#3.3_standardize)\n",
    "  * [3.3 Testing Training](#3.4_testing_training)\n",
    " * [3.2 Summary](#3.7_Summary)\n",
    "\n",
    "\n",
    "III. Split your data into testing and training datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75904d9f",
   "metadata": {},
   "source": [
    "## Training Data <a href=\"#2_Data_training_introduction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f8d7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Libraries\n"
     ]
    }
   ],
   "source": [
    "# data manipulation and math\n",
    "#\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "import pandas as pd\n",
    "#\n",
    "# plotting and visualization\n",
    "#\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.cm as cm\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#\n",
    "# modeling\n",
    "#\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "# import sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "print(\"Loaded Libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f25a1",
   "metadata": {},
   "source": [
    "### I. Training Data <a href=\"#2_Data_training_introduction\">\n",
    "Create dummy or indicator features for categorical variables (categories -- one hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33968c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9    ...  137  138  139  \\\n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "142  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "143  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "144  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "145  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "146  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
      "\n",
      "     140  141  142  143  144  145  146  \n",
      "0    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "1    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "2    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "3    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "4    0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
      "..   ...  ...  ...  ...  ...  ...  ...  \n",
      "142  0.0  0.0  1.0  0.0  0.0  0.0  0.0  \n",
      "143  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
      "144  0.0  0.0  0.0  0.0  1.0  0.0  0.0  \n",
      "145  0.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
      "146  0.0  0.0  0.0  0.0  0.0  0.0  1.0  \n",
      "\n",
      "[147 rows x 147 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "products = pd.read_csv(\"../data/processed/products.csv\")\n",
    "\n",
    "# New category list\n",
    "new_categories = pd.read_csv(\"../data/processed/Sunlight-Categories.csv\")[\"Category\"].tolist()  # Get the list of new categories\n",
    "\n",
    "# Convert the list to a numpy array\n",
    "new_categories_array = np.array(new_categories)\n",
    "\n",
    "# Reshape the array to have a single column\n",
    "new_categories_array = new_categories_array.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding for new categories\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "encoded_features = pd.DataFrame(encoder.fit_transform(new_categories_array))\n",
    "\n",
    "# Encoded features now contain columns for each new category\n",
    "print(encoded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323daa78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66db97b2",
   "metadata": {},
   "source": [
    "Standardize the magnitude of numeric features using a scaler (MSRP, Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61fd4538",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['MSRP', 'Size'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Extract numerical columns\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m numerical_cols \u001b[38;5;241m=\u001b[39m features[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSRP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize\u001b[39m\u001b[38;5;124m'\u001b[39m]]  \u001b[38;5;66;03m# Only select 'MSRP' and 'Size' columns\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Scale numerical features\u001b[39;00m\n\u001b[1;32m     20\u001b[0m scaled_numerical_cols \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(scaler\u001b[38;5;241m.\u001b[39mfit_transform(numerical_cols), columns\u001b[38;5;241m=\u001b[39mnumerical_cols\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:4096\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4094\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4095\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4096\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4098\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['MSRP', 'Size'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Select relevant features (optional)\n",
    "relevant_features = ['Description', 'Size', 'MSRP', 'Category'] + list(encoded_features.columns)\n",
    "features = pd.concat([pd.DataFrame(features).reset_index(drop=True), encoded_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define placeholder target column or based on some criteria\n",
    "# For example, let's say you want to predict whether a product is popular or not based on its MSRP\n",
    "# You can define the target column like this:\n",
    "products['NEWCategory'] = products['Category']\n",
    "\n",
    "# Define the target variable\n",
    "target = products['NEWCategory']\n",
    "\n",
    "# Standardize numeric features (if any)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Extract numerical columns\n",
    "numerical_cols = features[['MSRP', 'Size']]  # Only select 'MSRP' and 'Size' columns\n",
    "\n",
    "# Scale numerical features\n",
    "scaled_numerical_cols = pd.DataFrame(scaler.fit_transform(numerical_cols), columns=numerical_cols.columns)\n",
    "\n",
    "# Combine scaled numerical columns with non-scaled features\n",
    "scaled_features = pd.concat([scaled_numerical_cols, features.drop(['MSRP', 'Size'], axis=1)], axis=1)\n",
    "\n",
    "# Split data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now you have:\n",
    "# - X_train: Training features (scaled)\n",
    "# - X_test: Testing features (scaled)\n",
    "# - y_train: Training target variable\n",
    "# - y_test: Testing target variable\n",
    "\n",
    "# You can use these for model training and evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebb270",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "\n",
    "In general, categorical features need to be transformed or encoded to be used in some machine learning models, as is the case for Logistic Regression. A common transformation is so-called dummy encoding, where each possible value of a feature becomes a new column, and a 1 is placed in that column if the data instance (a row of the data) contained that value, and a 0 is placed in that column otherwise.\n",
    "\n",
    "For example, suppose we had a column in a hypothetical data set called species, and it contained one of two values, \"cat\" or \"dog\". The column might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4d938",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflog = pd.read_csv(\"../data/processed/products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32eacc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['Brand',\n",
    "                        'Size',\n",
    "                        'Color', \n",
    "                        'Description']\n",
    "'ParentCategory'\n",
    "'Category'\n",
    "dflog = pd.get_dummies(dflog, columns = categorical_features)\n",
    "print('The data have ', dflog.shape[0], ' rows and ', dflog.shape[1], ' columns\\n')\n",
    "print('column names: \\n')\n",
    "print('\\n'.join(list(dflog.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7ed53",
   "metadata": {},
   "source": [
    "#### Proportion of classes\n",
    "When building classification models, it is always a good idea to know right away the number of samples per class, proportionally to the total number of samples. First we get the counts of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329909bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = dflog['Category'].value_counts()\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_percentages = pd.Series([(x / dflog.shape[0]) * 100.00 for x in class_counts])\n",
    "class_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4971e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(class_counts.index, class_counts)\n",
    "# ax.set_xticks([0, 1])\n",
    "# ax.set_xticklabels(class_percentages.index.astype(str) + '\\n' + ' ' +\n",
    "#                    class_percentages.round(0).astype(str) + '%')\n",
    "# ax.set_ylabel('Count')\n",
    "# ax.set_xlabel('Category')\n",
    "# ax.set_title('Heart Disease class distribution\\nwhere 1 means presence of heart disease',\n",
    "#               fontsize = 10)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4e5909",
   "metadata": {},
   "source": [
    "Are there Imbalanced Multi-Class Classification Problems--IMCP going on here?? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a82da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plot(ax, Xtr, Xte, ytr, yte, \n",
    "                clf, \n",
    "                mesh = True, colorscale = cmap_light, cdiscrete = cmap_bold, \n",
    "                alpha = 0.1, psize = 10, \n",
    "                zfunc = False, predicted = False):\n",
    "#\n",
    "# note: this function only works for X.shape = (:, 2)\n",
    "# it is intended to illustrate the classifier boundary\n",
    "#\n",
    "# get the column names if they exist to apply\n",
    "# to the meshed data generated below\n",
    "#\n",
    "    try:\n",
    "        feature_names = Xtr.columns\n",
    "    except:\n",
    "        feature_names = None\n",
    "#        \n",
    "    Xtrain = np.array(Xtr)\n",
    "    Xtest = np.array(Xte)\n",
    "#\n",
    "    h = 0.02\n",
    "#\n",
    "# create a uniform grid spanning the range of the X values\n",
    "# note that y here is NOT the target, it is the 2nd\n",
    "# dimension of the desired plot\n",
    "#\n",
    "    X = np.concatenate((Xtrain, Xtest))\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "#\n",
    "# predict the target value at each point in the grid\n",
    "#\n",
    "# this method uses the probabilities from the classifier\n",
    "# and applies a function to determine the class\n",
    "#\n",
    "    if zfunc:\n",
    "        mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "        if feature_names is not None:\n",
    "            mesh_data = pd.DataFrame(mesh_data, \n",
    "                         columns = feature_names)\n",
    "        p0 = clf.predict_proba(mesh_data)[:, 0]\n",
    "        p1 = clf.predict_proba(mesh_data)[:, 1]\n",
    "        Z = zfunc(p0, p1)\n",
    "#\n",
    "# this method uses the classifier to predict the classes directly\n",
    "#\n",
    "    else:\n",
    "        mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "        if feature_names is not None:\n",
    "            mesh_data = pd.DataFrame(mesh_data, \n",
    "                                     columns = feature_names)\n",
    "        Z = clf.predict(mesh_data)\n",
    "    ZZ = Z.reshape(xx.shape)\n",
    "#\n",
    "# plt.pcolormesh() creates a shaded result over the grid\n",
    "#\n",
    "    if mesh:\n",
    "        plt.pcolormesh(xx, yy, ZZ, \n",
    "                       cmap = cmap_light, \n",
    "                       alpha = alpha, \n",
    "                       axes = ax, \n",
    "                       shading = 'auto')\n",
    "#\n",
    "# add the points to the plot\n",
    "# these can be the original target values\n",
    "# or the predicted values\n",
    "#\n",
    "    if predicted:\n",
    "        showtr = clf.predict(Xtr)\n",
    "        showte = clf.predict(Xte)\n",
    "    else:\n",
    "        showtr = ytr\n",
    "        showte = yte\n",
    "#\n",
    "# plot training points\n",
    "#\n",
    "    ax.scatter(Xtrain[:, 0], Xtrain[:, 1], \n",
    "               c = showtr - 1, \n",
    "               cmap = cmap_bold, \n",
    "               s = psize, \n",
    "               alpha = alpha, \n",
    "               edgecolor = \"k\")\n",
    "#    \n",
    "# plot testing points\n",
    "#\n",
    "    ax.scatter(Xtest[:, 0], Xtest[:, 1],\n",
    "               c = showte - 1, \n",
    "               cmap = cmap_bold, \n",
    "               s = psize + 10,\n",
    "               alpha = alpha, \n",
    "               marker = \"s\")\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "#\n",
    "    return ax, xx, yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3face",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plot_prob(ax, Xtr, Xte, ytr, yte, \n",
    "                     clf, colorscale = cmap_light, cdiscrete = cmap_bold, \n",
    "                     ccolor = cm, \n",
    "                     alpha = 0.1, psize = 10):\n",
    "    try:\n",
    "        feature_names = Xtr.columns\n",
    "    except:\n",
    "        feature_names = None\n",
    "#        \n",
    "    Xtrain = np.array(Xtr)\n",
    "    Xtest = np.array(Xte)\n",
    "#    \n",
    "    ax, xx, yy = points_plot(ax, Xtr, Xte, ytr, yte,\n",
    "                         clf,\n",
    "                         mesh = False, \n",
    "                         colorscale = colorscale, cdiscrete = cdiscrete, \n",
    "                         psize = psize, alpha = alpha,\n",
    "                         predicted = True) \n",
    "    mesh_data = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if feature_names is not None:\n",
    "        mesh_data = pd.DataFrame(mesh_data, \n",
    "                     columns = feature_names)    \n",
    "    Z = clf.predict_proba(mesh_data)[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap = ccolor, alpha = 0.2)\n",
    "    cs2 = plt.contour(xx, yy, Z, cmap = ccolor, alpha = 0.6)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize = 12)\n",
    "#\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d055c094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
